#+TITLE: Anomaly Explain
#+AUTHOR: Leilani H. Gilpin
#+EMAIL: lgilpin@ucsc.edu
#+STARTUP: hidestars indent

This is the anomaly detection through explanations (ADE)
architecture.  It compromises of two systems:
1. A system level architecture (ADE) which uses a synthesizer to
  reconcile failures amongst a set of reasonableness monitors.
2. Reasonableness monitors for each subsystem.
   
* Commonsense
The monitor supports commonsense from NextKB (companion_kb.py) and
ConceptNet (conceptnet.py).  

The commonsense data is stored as a list of facts (see Fact in
logical_classes.py) for a particular query or queries.  The list of
facts can be operated on as a list or as a dataframe.  

Run conceptNet tests with:
#+BEGIN_SRC
python3 system_tests/test_conceptnet.py -m unittest
#+END_SRC

** Reasonableness monitor
Reasonableness monitors are able to explain why a fact or list of
facts are reasonable.  They build up [[Commonsense]] knowledge, and
develop an explanation why a fact is reasonable or not.  These
monitors can be catered to a particular set of relations or not. 
** Synthesizer
* Tasks
** TODO Common representation for using facts
** TODO Chain together explanations
What we want (a,b,c) AND (c,e,f) -> a,b,c,e,f
penguins are birds and birds are animals -> penguins are animals.
- [ ] These are called "facts" (subject, predicate, object).
  - They are also stored in dataframes.  (But we go between them, that
    might not be the best idea).
- Lego example: waiting for better labels. 
** TODO Test suite
- Tests are scattered throughout the repo.  The most recent tests are
  in commonsense/tests and should probably be put somewhere else in
  the repo.
- There should also probably be one command to run all the tests (for
  the monitor, for commonsense, and the explainer).
- [ ] Small commonsense text file (so that if we don't have internet,
  it can still run).
  - Parse the conceptNet .csv file, and extract out 10-20 facts for
    testing.  
