{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanatory Anomaly Detection with ConceptNet and GloVe.\n",
    "\n",
    "This system demos the reasonableness monitoring system and anomaly detection through explanations with ConceptNet and GloVe.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t0/ny288p_d7dd56m7y37v_14fm0000gn/T/ipykernel_69584/1325287508.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeyedvectors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "#Imports Cell\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import gensim\n",
    "import pandas as pd\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put the path to glove here\n",
    "path = \"~/Dropbox (MIT)/car-can-explain/glove.6B.50d.txt.w2v\"\n",
    "\n",
    "#Now load the model into the variable \"glove\" (may take some time)\n",
    "glove = KeyedVectors.load_word2vec_format(path, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the reasonableness monitor\n",
    "import commonsense.conceptnet as kb\n",
    "import monitor.reasonableness_monitor as monitor\n",
    "import synthesizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove Embedding Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_embeddings(domain:set) -> list:\n",
    "    \"\"\"\n",
    "    Converts the domain of terms to a list of related embeddings\n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    domain: set\n",
    "    The set of terms that define the domain, each term is a string\n",
    "    \n",
    "    Returns\n",
    "    ---------\n",
    "    list of glove embeddings\n",
    "    \"\"\"\n",
    "    return [glove[word] for word in domain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function based on all the computations above\n",
    "def calcuate_distances(label_set_a:list, \n",
    "                       label_set_b:list) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    This function takes in two sets of glove embeddings vectors and returns the min distances between the two\n",
    "    \n",
    "    Parameters\n",
    "    -------------    \n",
    "    label_set_a : list \n",
    "            the first set of glove embedding vectors from one input source\n",
    "    label_set_b : list\n",
    "            the second set of glove embedding vectors from the second source\n",
    "    \n",
    "    Returns\n",
    "    ---------\n",
    "    numpy.ndarray\n",
    "        The list of distances, where length = max(len(label_set_a),len(label_set_b))\n",
    "    \"\"\"\n",
    "    \n",
    "    #Turn both into numpy arrays\n",
    "    arr_a = np.array(label_set_a)\n",
    "    arr_b = np.array(label_set_b)\n",
    "    \n",
    "    #Square and transform as needed\n",
    "    a = np.sum(arr_a**2,axis = 1)[:,np.newaxis]\n",
    "    b = np.sum(arr_b**2,axis = 1)\n",
    "    \n",
    "    #Calculate the distances and take the square root\n",
    "    #We are also cutting off where values too small\n",
    "    dists = a + b - 2*np.matmul(arr_a,arr_b.T)\n",
    "    dists[dists < 1e-6] = float(0.0)\n",
    "    dists = np.sqrt(dists)\n",
    "    \n",
    "    #Return the minimum values across the axis with more glove embeddings\n",
    "    return np.min(dists,axis = np.argmin(dists.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_domain(list_domain:list, depth:int) -> set:\n",
    "    \"\"\"\n",
    "    This function takes in a list of strings, which represents the domain, and generates the relavent list of glove embeddings that represents this domain. \n",
    "    \n",
    "    This does it through a recursive methodology\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    list_domain: list\n",
    "    List of string terms that represent the domain\n",
    "    \n",
    "    depth: int\n",
    "    How many layers should be used to generate the domain\n",
    "    \n",
    "    Returns\n",
    "    ---------\n",
    "    Set of strings of the terms that we should get the glove embeddings for\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    list_embeddings = set() #The final set representing the domain embeddings\n",
    "    \n",
    "    if depth == 0: #If we have gotten back to depth 0 it means we have added all the words to the depth we want\n",
    "        return list_embeddings\n",
    "    \n",
    "    \n",
    "    #We will use a BFS type function to generate our domain\n",
    "    queue = set(list_domain)\n",
    "    \n",
    "    for word in queue: #For each domain word\n",
    "        \n",
    "        #Add the first thing in the queue to our list\n",
    "        list_embeddings.add(word)\n",
    "        \n",
    "        #Get similar terms\n",
    "        list_terms = set(word for word, similarity in glove.similar_by_vector(glove[word]))\n",
    "        #Add its most similar words using a recursive call\n",
    "        list_embeddings.update(gen_domain(list_terms, depth - 1))\n",
    "    \n",
    "    return list_embeddings    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_df(concepts, domains):\n",
    "    \"\"\"\n",
    "    Takes in a list of concepts and a list of domains and outputs a pandas dataframe\n",
    "    of concept-domain glove score.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    domain = list(gen_domain(domains, 2))\n",
    "    domain_embeddings = convert_to_embeddings(domain)\n",
    "    \n",
    "    concept_embeddings = []\n",
    "    for concept in concepts: \n",
    "        concept_embeddings.append(convert_to_embeddings(concept))\n",
    "    \n",
    "    concept_dom_score = {}\n",
    "    \n",
    "    i = 0\n",
    "    for concept in concept_embeddings:\n",
    "        distances = list(calcuate_distances(concept, domain_embeddings))\n",
    "        concept_dom_score[concepts[i]] = []\n",
    "        j = 0\n",
    "        for score in distances:\n",
    "            concept_dom_score[concepts[i]].append([domain[j], score])\n",
    "            j += 1\n",
    "        i += 1\n",
    "    \n",
    "    \n",
    "    d = {'Word1':[], 'Relation':[], 'Word2':[], 'Score':[]}\n",
    "    \n",
    "    list_distances = list(distances)\n",
    "    \n",
    "    \n",
    "    for concept in concept_dom_score:\n",
    "        i=0\n",
    "        for score in concept_dom_score[concept]:   \n",
    "            d['Word1'].append(concept)\n",
    "            d['Relation'].append('glove')\n",
    "            d['Word2'].append(score[0])\n",
    "            d['Score'].append(score[1])\n",
    "            i += 1\n",
    "    \n",
    "    df = pd.DataFrame(data=d)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'find_anchor_with_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t0/ny288p_d7dd56m7y37v_14fm0000gn/T/ipykernel_69584/2093278284.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdescription\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'penguin crossing the street'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mconcepts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'penguin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'street'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mcommonsense_facts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcepts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mcommonsense_facts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/anomaly-explain/commonsense/conceptnet.py\u001b[0m in \u001b[0;36mbuild_df\u001b[0;34m(self, concepts, relations)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mfacts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mconcept\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconcepts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0mfacts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_anchor_with_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcept\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubject_anchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m             \u001b[0mfacts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_with_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcept\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfacts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Word1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Relation'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Word2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'find_anchor_with_score' is not defined"
     ]
    }
   ],
   "source": [
    "cn = kb.ConceptNet()\n",
    "\n",
    "description = 'penguin eats food'\n",
    "#concepts = ['penguin', 'food']\n",
    "description = 'penguin crossing the street'\n",
    "concepts = ['penguin', 'street']\n",
    "commonsense_facts = cn.build_df(concepts)\n",
    "commonsense_facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb.get_domain(commonsense_facts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered = commonsense_facts[commonsense_facts['Word1']=='a penguin']\n",
    "\n",
    "# kb.get_domain(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_facts = to_df(concepts,['zoo'])\n",
    "glove_facts = to_df(concepts, ['street'])\n",
    "glove_facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import synthesizer.argue as debate\n",
    "\n",
    "debate.challenge(commonsense_facts, glove_facts, description)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
