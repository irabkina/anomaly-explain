{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating Glove and ConceptNet\n",
    "\n",
    "This notebook is meant to demonstrate the features of Glove Embeddings to see how they can potentially be used in conjunction with the COCO dataset to numerically analyze explanations and such. The link to download the embeddings is [here](https://nlp.stanford.edu/projects/glove), and I downloaded the **6b** one with Wikipedia and all.\n",
    "\n",
    "[Link to Glove Paper](https://nlp.stanford.edu/pubs/glove.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports that may be Necessary\n",
    "\n",
    "```python\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "```\n",
    "*Note: You may need to conda install gensim*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports Cell\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the path to glove here\n",
    "# TODO: we may want to put a better pointer here.  \n",
    "file_name = \"glove.6B.50d.txt.w2v\"\n",
    "path = f\"../glove/{file_name}\"\n",
    "\n",
    "#Now load the model into the variable \"glove\" (may take some time)\n",
    "glove = KeyedVectors.load_word2vec_format(path, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Use Glove\n",
    "```python\n",
    "glove[\"word\"] # Will give glove embedding vector for the word\n",
    "\n",
    "\"word\" in glove #Checks if word is in glove (acts like a dictionary\n",
    "\n",
    "glove[\"husband\"] - glove[\"man\"] + glove[\"woman\"] #Should give representation that is wife\n",
    "\n",
    "#To find most similar term to a vector:\n",
    "    \n",
    "glove.similar_by_vector(query)\n",
    "\n",
    "#More advanced way to do this\n",
    "\n",
    "glove.most_similar_cosmul(positive=['husband', 'woman'], negative=['man'])\n",
    "\n",
    "#Since they are vectors, we can find the distance using dot products\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('stoplight', 1.0000001192092896),\n",
       " ('flyspeck', 0.6498768329620361),\n",
       " ('one-horse', 0.6459125876426697),\n",
       " ('18-wheeler', 0.6452605128288269),\n",
       " ('sea-side', 0.615469217300415),\n",
       " ('nanopores', 0.613706111907959),\n",
       " ('tepee', 0.6063854694366455),\n",
       " ('12-block', 0.6035683751106262),\n",
       " ('abobo', 0.5967516303062439),\n",
       " ('hurriyah', 0.5955226421356201)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define our domains based on the labels for searching the images\n",
    "#Suppose \"car\" and \"train\" domain\n",
    "# Iteratively find the most similar (ex. depth of 2 = for each main label, also add all their friend labels)\n",
    "concept = \"stoplight\"\n",
    "glove.similar_by_vector(glove[concept])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to the reasonableness monitor \n",
    "import argparse\n",
    "import requests\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "import nltk\n",
    "from sympy import *  # python symbolic package\n",
    "from sympy.logic import SOPform\n",
    "import itertools\n",
    "from time import process_time\n",
    "\n",
    "# TODO we may want to do some data science stuff \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "#import commonsense.conceptnet as kb\n",
    "import synthesizer.synthesize as synthesize\n",
    "\n",
    "import monitor.reasonableness_monitor as monitor\n",
    "anchors = ['animal', 'object', 'place', 'plant']\n",
    "relations = ['AtLocation', 'LocatedNear'] # technically these are vehicle relations for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Simple parser entity extraction for the captions and their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REASONS ARE [['stoplight IsA object', 'Default anchor point']]\n",
      "REASONS ARE [['stoplight IsA object', 'Default anchor point']]\n",
      "[['stoplight IsA object', 'Default anchor point']]\n",
      "Are we here in reasonable ['stoplight']\n"
     ]
    }
   ],
   "source": [
    "stoplightExplain = monitor.snapshot_monitor([concept], anchors, relations, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Ideas for Symbolic Reasoning\n",
    "\n",
    "Suppose we had a bunch of **labels** from each of the outputs for the subsystems (the sem. seg & the two captions), we try to figure out how close all the different labels are across different systems.\n",
    "\n",
    "We figure out a threshold and if 2-3 of them are super close in their vectors, and another one is not, we suspect that one, generally, but here are some other general ideas:\n",
    "\n",
    "- Since each subsystem will present its own set of labels, (all the labels must be relatively close to each other) if **any of them seem abnormaly far away from the others** (maybe we can scramble to see this), then we say that it is not reasonable\n",
    "- We do the same thing across multiple ones as well, and see the distances (maybe min distances) and try to figure out at a high level who is not reasonable\n",
    "- We combine these local, and high-level checks with symbolic checks to determine overall reasonability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Demonstration\n",
    "\n",
    "Let's take 3 different images from the CoCo Dataset where:\n",
    "\n",
    "- Two of the images will be similar, and 1 image will be different\n",
    "\n",
    "We will see if we can use the **semantic segmentation tags** + some basic distance calculations to see which images should be closest to each other:\n",
    "\n",
    "Links to the three images used:\n",
    "\n",
    "- \n",
    "- \n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math for Calculating Distances Between Captions\n",
    "\n",
    "Suppose we have **sem. seg** labels that identify **O** objects, assuming we are using **N** dimensional vectors, we have a (**O**x**N**) array representing our values. \n",
    "\n",
    "Now suppose we have **caption labels** that identify **K** objects, assuming we are still using **N** dimensional vectors, we have a (**K**x**N**) array representing our values. \n",
    "\n",
    "## Finding Distances\n",
    "\n",
    "When we are trying to find the distances, we need to use **multiplication** to make this easy. However, we need to remember that order matters if we were to just blindly do it:\n",
    "\n",
    "```python\n",
    "[[man], [woman], [cat]] * [[man], [woman], [cat]] = 0 #As distances between each element and itself would be zero\n",
    "\n",
    "#However\n",
    "[[cat], [woman], [man]] * [[man], [woman], [cat]] !=0 #As order is different so what will get multiplied is different\n",
    "```\n",
    "\n",
    "Therefore we will specifically use **matrix multiplication**. Therefore, the steps to find distances based on this are:\n",
    "\n",
    "\n",
    "The python code to do so is as follows (uses `numpy`):\n",
    "\n",
    "```python\n",
    "O = 7\n",
    "N = 2\n",
    "K = 6\n",
    "\n",
    "#Create two matrixes based on dimensions\n",
    "x = np.arange(14).reshape((O,N))\n",
    "y = np.arange(12).reshape((K,N))\n",
    "\n",
    "# Distances are x^2 + y^2 - 2*x*y\n",
    "dists = np.sum(x**2, axis=1)[:,np.newaxis] + np.sum(y**2, axis=1) \n",
    "dists -= 2*np.matmul(x,y.T)\n",
    "\n",
    "distances = np.sqrt(dists)\n",
    "\n",
    "#Now we find the minimum of this to represent as our distance\n",
    "#For the axis, select whichever axis is smaller (will either be zero or 1)\n",
    "\n",
    "\n",
    "np.min(distances,axis = np.argmin(distances.shape))\n",
    "\n",
    "#By doing the above, we cover the one with more objects, so that there is a greater chance of greater distances (vs not properly accounting for an object's distance to others. Though, it shouldn't really matter either way.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_a = np.array([glove[\"man\"],glove[\"woman\"],glove[\"cat\"]])\n",
    "arr_b = np.array([glove[\"man\"],glove[\"woman\"],glove[\"cat\"],glove[\"dog\"]])\n",
    "\n",
    "a = np.sum(arr_a**2,axis = 1)[:,np.newaxis]\n",
    "b = np.sum(arr_b**2,axis = 1)\n",
    "\n",
    "dists = a + b - 2*np.matmul(arr_a,arr_b.T)\n",
    "dists[dists < 1e-6] = float(0.0)\n",
    "dists = np.sqrt(dists)\n",
    "type(np.min(dists,axis = np.argmin(dists.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function based on all the computations above\n",
    "def calcuate_distances(label_set_a:list, \n",
    "                       label_set_b:list) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    This function takes in two sets of glove embeddings vectors and returns the min distances between the two\n",
    "    \n",
    "    Parameters\n",
    "    -------------    \n",
    "    label_set_a : list \n",
    "            the first set of glove embedding vectors from one input source\n",
    "    label_set_b : list\n",
    "            the second set of glove embedding vectors from the second source\n",
    "    \n",
    "    Returns\n",
    "    ---------\n",
    "    numpy.ndarray\n",
    "        The list of distances, where length = max(len(label_set_a),len(label_set_b))\n",
    "    \"\"\"\n",
    "    \n",
    "    #Turn both into numpy arrays\n",
    "    arr_a = np.array(label_set_a)\n",
    "    arr_b = np.array(label_set_b)\n",
    "    \n",
    "    #Square and transform as needed\n",
    "    a = np.sum(arr_a**2,axis = 1)[:,np.newaxis]\n",
    "    b = np.sum(arr_b**2,axis = 1)\n",
    "    \n",
    "    #Calculate the distances and take the square root\n",
    "    #We are also cutting off where values too small\n",
    "    dists = a + b - 2*np.matmul(arr_a,arr_b.T)\n",
    "    dists[dists < 1e-6] = float(0.0)\n",
    "    dists = np.sqrt(dists)\n",
    "    \n",
    "    #Return the minimum values across the axis with more glove embeddings\n",
    "    return np.min(dists,axis = np.argmin(dists.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function based on all the computations above\n",
    "def calcuate_distance(label_set_a:list, \n",
    "                       label_set_b:list) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    This function takes in two sets of glove embeddings vectors and returns a single value representing the distance between the two values\n",
    "    \n",
    "    Parameters\n",
    "    -------------    \n",
    "    label_set_a : list \n",
    "            the first set of glove embedding vectors from one input source\n",
    "    label_set_b : list\n",
    "            the second set of glove embedding vectors from the second source\n",
    "    \n",
    "    Returns\n",
    "    ---------\n",
    "    float32\n",
    "        A single value representing the distance between label_set_a and label_set_b\n",
    "    \"\"\"\n",
    "    \n",
    "    #Turn both into numpy arrays\n",
    "    arr_a = np.array(label_set_a)\n",
    "    arr_b = np.array(label_set_b)\n",
    "    \n",
    "    #Square and transform as needed\n",
    "    a = np.sum(arr_a**2,axis = 1)[:,np.newaxis]\n",
    "    b = np.sum(arr_b**2,axis = 1)\n",
    "    \n",
    "    #Calculate the distances and take the square root\n",
    "    #We are also cutting off where values too small\n",
    "    dists = a + b - 2*np.matmul(arr_a,arr_b.T)\n",
    "    dists[dists < 1e-6] = float(0.0)\n",
    "    dists = np.sqrt(dists)\n",
    "    \n",
    "    #Return the minimum values across the axis with more glove embeddings\n",
    "    return np.sum(np.min(dists,axis = np.argmin(dists.shape)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's use our above function with labels of 2 similar images and 1 different\n",
    "\n",
    "For this test, we will simply **only use the sem.seg labels to see how well we can tell distances with that** (idea is that captions will use similar idea):\n",
    "- [Similar Image A](https://cocodataset.org/#explore?id=5253) and [Similar Image B](https://cocodataset.org/#explore?id=277614) selected on coco site by clicking *stop sign* (stop) and *traffic light* (stoplight)\n",
    "- [Different Image A](https://cocodataset.org/#explore?id=360877) selected on coco site by clicking *apple* and *chair*\n",
    "\n",
    "Let's see the 3 way comparison test for distances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Code to manually look through available words\n",
    "# asd = list(glove.vocab.keys())\n",
    "# asd.sort()\n",
    "#print(asd[360000:370000])\n",
    "\"traffic-sign\" in glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of representaion based on sem. seg labels\n",
    "similar_image_a = [glove[\"stop\"],glove[\"stoplight\"]]\n",
    "similar_image_b = [glove[\"stop\"],glove[\"stoplight\"],glove[\"train\"],glove[\"clock\"]]\n",
    "different_image_a = [glove[\"person\"],glove[\"bottle\"],glove[\"banana\"],glove[\"apple\"],glove[\"chair\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distances between similar_image_a, similar_image_b: \n",
      "[0.        0.        3.8940194 4.6098065]\n",
      "\n",
      "\n",
      "Distances between similar_image_a, different_image_a: \n",
      "[4.951073  5.6056447 5.913838  6.0824575 5.8651605]\n",
      "\n",
      "\n",
      "Distances between similar_image_b, different_image_a: \n",
      "[4.951073  5.6056447 5.913838  5.834715  4.8754997]\n",
      "\n",
      "\n",
      "Printing the sums of each:\n",
      "8.503826\n",
      "28.418173\n",
      "27.18077\n"
     ]
    }
   ],
   "source": [
    "print(\"Distances between similar_image_a, similar_image_b: \")\n",
    "print(calcuate_distances(similar_image_a,similar_image_b))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Distances between similar_image_a, different_image_a: \")\n",
    "print(calcuate_distances(similar_image_a,different_image_a))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Distances between similar_image_b, different_image_a: \")\n",
    "print(calcuate_distances(similar_image_b,different_image_a))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Printing the sums of each:\")\n",
    "print(calcuate_distance(similar_image_a,similar_image_b))\n",
    "print(calcuate_distance(similar_image_a,different_image_a))\n",
    "print(calcuate_distance(similar_image_b,different_image_a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we will try the same process as above but with scrambling some of the ordering (not making it nice and uniform across the diff. inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of representaion based on sem. seg labels but with different ordering\n",
    "similar_image_a = [glove[\"stoplight\"], glove[\"stop\"]]\n",
    "similar_image_b = [glove[\"train\"], glove[\"stop\"], glove[\"stoplight\"], glove[\"clock\"]]\n",
    "different_image_a = [glove[\"bottle\"],glove[\"banana\"],glove[\"person\"],glove[\"chair\"], glove[\"apple\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distances between similar_image_a, similar_image_b: \n",
      "[3.8940194 0.        0.        4.6098065]\n",
      "\n",
      "\n",
      "Distances between similar_image_a, different_image_a: \n",
      "[5.6056447 5.913838  4.951073  5.8651605 6.0824575]\n",
      "\n",
      "\n",
      "Distances between similar_image_b, different_image_a: \n",
      "[5.6056447 5.913838  4.951073  4.8754997 5.834715 ]\n",
      "\n",
      "\n",
      "Printing the sums of each:\n",
      "8.503826\n",
      "28.418175\n",
      "27.18077\n"
     ]
    }
   ],
   "source": [
    "print(\"Distances between similar_image_a, similar_image_b: \")\n",
    "print(calcuate_distances(similar_image_a,similar_image_b))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Distances between similar_image_a, different_image_a: \")\n",
    "print(calcuate_distances(similar_image_a,different_image_a))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Distances between similar_image_b, different_image_a: \")\n",
    "print(calcuate_distances(similar_image_b,different_image_a))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Printing the sums of each:\")\n",
    "print(calcuate_distance(similar_image_a,similar_image_b))\n",
    "print(calcuate_distance(similar_image_a,different_image_a))\n",
    "print(calcuate_distance(similar_image_b,different_image_a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "As you can see from the cell above, the two *similar images* were close to each other, **and had similar distances to the different image** which can be really useful for **outlier analysis**. \n",
    "\n",
    "Another great thing that you can see is that *independent of the order of the different label input* **the outputs for the overall distances were identical** (and the vectors just in a different ordering, same values)\n",
    "\n",
    "- We can also try this with higher dimensional (this was just with 50D vectors) so higher dimensional might lead to even tighter distances (closer things are closer, farther things are farther)\n",
    "- We can also try this process and calculate distances **within 1 single image label set to find outlier labels** (test with scrambling labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labels within a Singular Object Distance\n",
    "\n",
    "Now let's see what happens when we scramble labels within a single image and see if there is any possible way to identify that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.94971    0.34328    0.84504   -0.88519   -0.72078   -0.29309\n",
      " -0.74678    0.65122    0.47295   -0.74011    0.1877    -0.38279\n",
      " -0.55899    0.42952   -0.26984   -0.42383   -0.31236    1.3423\n",
      " -0.78567   -0.6302     0.91819    0.21126   -0.57442    1.4549\n",
      "  0.75456   -1.6165    -0.0085015  0.0029134  0.51304   -0.47447\n",
      "  2.5306     0.85944   -0.30667    0.057765   0.66231    0.20804\n",
      "  0.64237   -0.5246    -0.053416   1.1404    -0.13703   -0.18361\n",
      "  0.45459   -0.50963   -0.025539  -0.02861    0.18048   -0.4483\n",
      "  0.40525   -0.36821  ]\n",
      "[ 0.94971    0.34328    0.84504   -0.88519   -0.72078   -0.29309\n",
      " -0.74678    0.65122    0.47295   -0.74011    0.1877    -0.38279\n",
      " -0.55899    0.42952   -0.26984   -0.42383   -0.31236    1.3423\n",
      " -0.78567   -0.6302     0.91819    0.21126   -0.57442    1.4549\n",
      "  0.75456   -1.6165    -0.0085015  0.0029134  0.51304   -0.47447\n",
      "  2.5306     0.85944   -0.30667    0.057765   0.66231    0.20804\n",
      "  0.64237   -0.5246    -0.053416   1.1404    -0.13703   -0.18361\n",
      "  0.45459   -0.50963   -0.025539  -0.02861    0.18048   -0.4483\n",
      "  0.40525   -0.36821  ]\n",
      "1.0\n",
      "[ 0.94971    0.34328    0.84504   -0.88519   -0.72078   -0.29309\n",
      " -0.74678    0.65122    0.47295   -0.74011    0.1877    -0.38279\n",
      " -0.55899    0.42952   -0.26984   -0.42383   -0.31236    1.3423\n",
      " -0.78567   -0.6302     0.91819    0.21126   -0.57442    1.4549\n",
      "  0.75456   -1.6165    -0.0085015  0.0029134  0.51304   -0.47447\n",
      "  2.5306     0.85944   -0.30667    0.057765   0.66231    0.20804\n",
      "  0.64237   -0.5246    -0.053416   1.1404    -0.13703   -0.18361\n",
      "  0.45459   -0.50963   -0.025539  -0.02861    0.18048   -0.4483\n",
      "  0.40525   -0.36821  ]\n",
      "[ 3.1973e-01 -5.3511e-01  1.6837e-01 -5.1493e-01 -5.4168e-01 -5.2323e-02\n",
      " -3.4670e-01  4.1141e-01  4.8828e-01  1.9277e-01 -4.3457e-01 -5.4993e-02\n",
      " -7.0818e-01 -2.1076e-01  3.0517e-01  6.3560e-02  3.5297e-01 -1.2412e-01\n",
      " -1.2177e-01 -4.1381e-01  7.4801e-01  2.1710e-01  7.7925e-03  5.0231e-01\n",
      "  2.4590e-01 -2.2516e+00  5.8344e-01  1.2077e-01  1.2825e+00 -1.0129e+00\n",
      "  2.9238e+00  7.5870e-01 -1.0051e+00 -1.7133e-01 -6.1737e-01 -1.5465e-02\n",
      "  3.0681e-01 -7.0980e-01 -3.9136e-01  5.3807e-01 -1.5420e-01  8.3077e-02\n",
      " -2.0523e-03 -1.1419e-01  6.6978e-01 -2.2367e-01 -1.6531e-01 -2.1139e-02\n",
      "  6.1458e-02 -1.0773e-01]\n",
      "0.699975848197937\n",
      "[ 0.94971    0.34328    0.84504   -0.88519   -0.72078   -0.29309\n",
      " -0.74678    0.65122    0.47295   -0.74011    0.1877    -0.38279\n",
      " -0.55899    0.42952   -0.26984   -0.42383   -0.31236    1.3423\n",
      " -0.78567   -0.6302     0.91819    0.21126   -0.57442    1.4549\n",
      "  0.75456   -1.6165    -0.0085015  0.0029134  0.51304   -0.47447\n",
      "  2.5306     0.85944   -0.30667    0.057765   0.66231    0.20804\n",
      "  0.64237   -0.5246    -0.053416   1.1404    -0.13703   -0.18361\n",
      "  0.45459   -0.50963   -0.025539  -0.02861    0.18048   -0.4483\n",
      "  0.40525   -0.36821  ]\n",
      "[ 5.0625e-01 -7.5861e-02  3.5667e-01 -1.9916e-01  4.2828e-01 -4.9580e-01\n",
      "  1.7988e-02  5.0164e-01 -7.4663e-02  1.6919e-02 -8.8081e-01 -1.6071e+00\n",
      "  4.0707e-01  1.9289e-01 -3.0754e-01  1.1850e-01 -1.6106e-04  7.4773e-01\n",
      "  1.3391e-01 -4.6638e-01 -6.7023e-01 -3.0118e-01 -3.0377e-01  1.2345e+00\n",
      "  9.0100e-02  1.3800e-01 -3.3182e-01  1.4311e+00  9.8952e-01 -4.4896e-01\n",
      " -4.5327e-01  2.9784e-01 -5.6369e-01  8.7011e-01  1.0898e+00 -1.8810e-03\n",
      "  4.1680e-01 -3.0524e-01  1.1515e-02  6.4577e-01 -2.9024e-01  3.0183e-01\n",
      " -1.0317e+00  2.7942e-02 -5.0534e-01 -5.4613e-01  6.9852e-01 -3.7518e-01\n",
      "  6.7727e-01 -7.4941e-02]\n",
      "0.2701544761657715\n",
      "[ 0.94971    0.34328    0.84504   -0.88519   -0.72078   -0.29309\n",
      " -0.74678    0.65122    0.47295   -0.74011    0.1877    -0.38279\n",
      " -0.55899    0.42952   -0.26984   -0.42383   -0.31236    1.3423\n",
      " -0.78567   -0.6302     0.91819    0.21126   -0.57442    1.4549\n",
      "  0.75456   -1.6165    -0.0085015  0.0029134  0.51304   -0.47447\n",
      "  2.5306     0.85944   -0.30667    0.057765   0.66231    0.20804\n",
      "  0.64237   -0.5246    -0.053416   1.1404    -0.13703   -0.18361\n",
      "  0.45459   -0.50963   -0.025539  -0.02861    0.18048   -0.4483\n",
      "  0.40525   -0.36821  ]\n",
      "[-2.7866e-01 -2.9872e-01  6.6275e-01 -9.5487e-02  5.2230e-01 -4.3949e-01\n",
      "  3.1018e-02 -2.6427e-01  3.3310e-01 -2.1021e-01  1.0597e-01 -5.5006e-01\n",
      " -2.3407e-01  1.0332e+00  1.7066e-01  7.3381e-01 -5.9416e-01 -1.6251e-01\n",
      " -8.4043e-01 -7.2089e-01  6.8816e-01 -2.0450e-01 -7.5548e-01 -1.1226e-01\n",
      "  9.6886e-01 -6.6068e-01 -8.9874e-02 -1.0576e-01  8.8008e-01 -3.2770e-01\n",
      "  2.1922e+00 -4.8859e-02 -4.8141e-01 -1.6170e-01 -2.0312e-01  2.7131e-01\n",
      "  1.1705e+00  7.6043e-05  2.2993e-01  7.0042e-01 -1.6987e-01 -5.4231e-02\n",
      " -9.3794e-01  8.7418e-01 -1.9089e-01  3.4681e-01  1.2777e+00 -4.2743e-01\n",
      " -3.2283e-01 -2.4891e-01]\n",
      "0.5137549638748169\n",
      "[ 0.94971    0.34328    0.84504   -0.88519   -0.72078   -0.29309\n",
      " -0.74678    0.65122    0.47295   -0.74011    0.1877    -0.38279\n",
      " -0.55899    0.42952   -0.26984   -0.42383   -0.31236    1.3423\n",
      " -0.78567   -0.6302     0.91819    0.21126   -0.57442    1.4549\n",
      "  0.75456   -1.6165    -0.0085015  0.0029134  0.51304   -0.47447\n",
      "  2.5306     0.85944   -0.30667    0.057765   0.66231    0.20804\n",
      "  0.64237   -0.5246    -0.053416   1.1404    -0.13703   -0.18361\n",
      "  0.45459   -0.50963   -0.025539  -0.02861    0.18048   -0.4483\n",
      "  0.40525   -0.36821  ]\n",
      "[ 0.044751  0.22425   0.74523  -0.3927    0.73664  -0.086284 -0.010413\n",
      "  0.41628   0.5249   -0.16484   0.29205  -0.095553  0.61566   1.602\n",
      "  0.4641    1.2693   -1.1416    0.88237   0.47702  -0.78944   1.5937\n",
      " -0.34953   0.42561   0.56537  -0.32054  -0.5776   -0.39622   1.129\n",
      "  1.8284   -0.10004   0.82675   0.46756  -0.23037   0.80698   0.52753\n",
      "  1.0272    0.21814   1.1406    1.4022    0.30931   0.57774   0.2999\n",
      " -0.21302  -0.046468  0.076242  0.91611   0.47489  -0.55653  -0.45134\n",
      "  0.049276]\n",
      "0.3487786650657654\n",
      "[ 3.1973e-01 -5.3511e-01  1.6837e-01 -5.1493e-01 -5.4168e-01 -5.2323e-02\n",
      " -3.4670e-01  4.1141e-01  4.8828e-01  1.9277e-01 -4.3457e-01 -5.4993e-02\n",
      " -7.0818e-01 -2.1076e-01  3.0517e-01  6.3560e-02  3.5297e-01 -1.2412e-01\n",
      " -1.2177e-01 -4.1381e-01  7.4801e-01  2.1710e-01  7.7925e-03  5.0231e-01\n",
      "  2.4590e-01 -2.2516e+00  5.8344e-01  1.2077e-01  1.2825e+00 -1.0129e+00\n",
      "  2.9238e+00  7.5870e-01 -1.0051e+00 -1.7133e-01 -6.1737e-01 -1.5465e-02\n",
      "  3.0681e-01 -7.0980e-01 -3.9136e-01  5.3807e-01 -1.5420e-01  8.3077e-02\n",
      " -2.0523e-03 -1.1419e-01  6.6978e-01 -2.2367e-01 -1.6531e-01 -2.1139e-02\n",
      "  6.1458e-02 -1.0773e-01]\n",
      "[ 0.94971    0.34328    0.84504   -0.88519   -0.72078   -0.29309\n",
      " -0.74678    0.65122    0.47295   -0.74011    0.1877    -0.38279\n",
      " -0.55899    0.42952   -0.26984   -0.42383   -0.31236    1.3423\n",
      " -0.78567   -0.6302     0.91819    0.21126   -0.57442    1.4549\n",
      "  0.75456   -1.6165    -0.0085015  0.0029134  0.51304   -0.47447\n",
      "  2.5306     0.85944   -0.30667    0.057765   0.66231    0.20804\n",
      "  0.64237   -0.5246    -0.053416   1.1404    -0.13703   -0.18361\n",
      "  0.45459   -0.50963   -0.025539  -0.02861    0.18048   -0.4483\n",
      "  0.40525   -0.36821  ]\n",
      "0.699975848197937\n",
      "[ 3.1973e-01 -5.3511e-01  1.6837e-01 -5.1493e-01 -5.4168e-01 -5.2323e-02\n",
      " -3.4670e-01  4.1141e-01  4.8828e-01  1.9277e-01 -4.3457e-01 -5.4993e-02\n",
      " -7.0818e-01 -2.1076e-01  3.0517e-01  6.3560e-02  3.5297e-01 -1.2412e-01\n",
      " -1.2177e-01 -4.1381e-01  7.4801e-01  2.1710e-01  7.7925e-03  5.0231e-01\n",
      "  2.4590e-01 -2.2516e+00  5.8344e-01  1.2077e-01  1.2825e+00 -1.0129e+00\n",
      "  2.9238e+00  7.5870e-01 -1.0051e+00 -1.7133e-01 -6.1737e-01 -1.5465e-02\n",
      "  3.0681e-01 -7.0980e-01 -3.9136e-01  5.3807e-01 -1.5420e-01  8.3077e-02\n",
      " -2.0523e-03 -1.1419e-01  6.6978e-01 -2.2367e-01 -1.6531e-01 -2.1139e-02\n",
      "  6.1458e-02 -1.0773e-01]\n",
      "[ 3.1973e-01 -5.3511e-01  1.6837e-01 -5.1493e-01 -5.4168e-01 -5.2323e-02\n",
      " -3.4670e-01  4.1141e-01  4.8828e-01  1.9277e-01 -4.3457e-01 -5.4993e-02\n",
      " -7.0818e-01 -2.1076e-01  3.0517e-01  6.3560e-02  3.5297e-01 -1.2412e-01\n",
      " -1.2177e-01 -4.1381e-01  7.4801e-01  2.1710e-01  7.7925e-03  5.0231e-01\n",
      "  2.4590e-01 -2.2516e+00  5.8344e-01  1.2077e-01  1.2825e+00 -1.0129e+00\n",
      "  2.9238e+00  7.5870e-01 -1.0051e+00 -1.7133e-01 -6.1737e-01 -1.5465e-02\n",
      "  3.0681e-01 -7.0980e-01 -3.9136e-01  5.3807e-01 -1.5420e-01  8.3077e-02\n",
      " -2.0523e-03 -1.1419e-01  6.6978e-01 -2.2367e-01 -1.6531e-01 -2.1139e-02\n",
      "  6.1458e-02 -1.0773e-01]\n",
      "1.0\n",
      "[ 3.1973e-01 -5.3511e-01  1.6837e-01 -5.1493e-01 -5.4168e-01 -5.2323e-02\n",
      " -3.4670e-01  4.1141e-01  4.8828e-01  1.9277e-01 -4.3457e-01 -5.4993e-02\n",
      " -7.0818e-01 -2.1076e-01  3.0517e-01  6.3560e-02  3.5297e-01 -1.2412e-01\n",
      " -1.2177e-01 -4.1381e-01  7.4801e-01  2.1710e-01  7.7925e-03  5.0231e-01\n",
      "  2.4590e-01 -2.2516e+00  5.8344e-01  1.2077e-01  1.2825e+00 -1.0129e+00\n",
      "  2.9238e+00  7.5870e-01 -1.0051e+00 -1.7133e-01 -6.1737e-01 -1.5465e-02\n",
      "  3.0681e-01 -7.0980e-01 -3.9136e-01  5.3807e-01 -1.5420e-01  8.3077e-02\n",
      " -2.0523e-03 -1.1419e-01  6.6978e-01 -2.2367e-01 -1.6531e-01 -2.1139e-02\n",
      "  6.1458e-02 -1.0773e-01]\n",
      "[ 5.0625e-01 -7.5861e-02  3.5667e-01 -1.9916e-01  4.2828e-01 -4.9580e-01\n",
      "  1.7988e-02  5.0164e-01 -7.4663e-02  1.6919e-02 -8.8081e-01 -1.6071e+00\n",
      "  4.0707e-01  1.9289e-01 -3.0754e-01  1.1850e-01 -1.6106e-04  7.4773e-01\n",
      "  1.3391e-01 -4.6638e-01 -6.7023e-01 -3.0118e-01 -3.0377e-01  1.2345e+00\n",
      "  9.0100e-02  1.3800e-01 -3.3182e-01  1.4311e+00  9.8952e-01 -4.4896e-01\n",
      " -4.5327e-01  2.9784e-01 -5.6369e-01  8.7011e-01  1.0898e+00 -1.8810e-03\n",
      "  4.1680e-01 -3.0524e-01  1.1515e-02  6.4577e-01 -2.9024e-01  3.0183e-01\n",
      " -1.0317e+00  2.7942e-02 -5.0534e-01 -5.4613e-01  6.9852e-01 -3.7518e-01\n",
      "  6.7727e-01 -7.4941e-02]\n",
      "0.05217964947223663\n",
      "[ 3.1973e-01 -5.3511e-01  1.6837e-01 -5.1493e-01 -5.4168e-01 -5.2323e-02\n",
      " -3.4670e-01  4.1141e-01  4.8828e-01  1.9277e-01 -4.3457e-01 -5.4993e-02\n",
      " -7.0818e-01 -2.1076e-01  3.0517e-01  6.3560e-02  3.5297e-01 -1.2412e-01\n",
      " -1.2177e-01 -4.1381e-01  7.4801e-01  2.1710e-01  7.7925e-03  5.0231e-01\n",
      "  2.4590e-01 -2.2516e+00  5.8344e-01  1.2077e-01  1.2825e+00 -1.0129e+00\n",
      "  2.9238e+00  7.5870e-01 -1.0051e+00 -1.7133e-01 -6.1737e-01 -1.5465e-02\n",
      "  3.0681e-01 -7.0980e-01 -3.9136e-01  5.3807e-01 -1.5420e-01  8.3077e-02\n",
      " -2.0523e-03 -1.1419e-01  6.6978e-01 -2.2367e-01 -1.6531e-01 -2.1139e-02\n",
      "  6.1458e-02 -1.0773e-01]\n",
      "[-2.7866e-01 -2.9872e-01  6.6275e-01 -9.5487e-02  5.2230e-01 -4.3949e-01\n",
      "  3.1018e-02 -2.6427e-01  3.3310e-01 -2.1021e-01  1.0597e-01 -5.5006e-01\n",
      " -2.3407e-01  1.0332e+00  1.7066e-01  7.3381e-01 -5.9416e-01 -1.6251e-01\n",
      " -8.4043e-01 -7.2089e-01  6.8816e-01 -2.0450e-01 -7.5548e-01 -1.1226e-01\n",
      "  9.6886e-01 -6.6068e-01 -8.9874e-02 -1.0576e-01  8.8008e-01 -3.2770e-01\n",
      "  2.1922e+00 -4.8859e-02 -4.8141e-01 -1.6170e-01 -2.0312e-01  2.7131e-01\n",
      "  1.1705e+00  7.6043e-05  2.2993e-01  7.0042e-01 -1.6987e-01 -5.4231e-02\n",
      " -9.3794e-01  8.7418e-01 -1.9089e-01  3.4681e-01  1.2777e+00 -4.2743e-01\n",
      " -3.2283e-01 -2.4891e-01]\n",
      "0.5098719000816345\n",
      "[ 3.1973e-01 -5.3511e-01  1.6837e-01 -5.1493e-01 -5.4168e-01 -5.2323e-02\n",
      " -3.4670e-01  4.1141e-01  4.8828e-01  1.9277e-01 -4.3457e-01 -5.4993e-02\n",
      " -7.0818e-01 -2.1076e-01  3.0517e-01  6.3560e-02  3.5297e-01 -1.2412e-01\n",
      " -1.2177e-01 -4.1381e-01  7.4801e-01  2.1710e-01  7.7925e-03  5.0231e-01\n",
      "  2.4590e-01 -2.2516e+00  5.8344e-01  1.2077e-01  1.2825e+00 -1.0129e+00\n",
      "  2.9238e+00  7.5870e-01 -1.0051e+00 -1.7133e-01 -6.1737e-01 -1.5465e-02\n",
      "  3.0681e-01 -7.0980e-01 -3.9136e-01  5.3807e-01 -1.5420e-01  8.3077e-02\n",
      " -2.0523e-03 -1.1419e-01  6.6978e-01 -2.2367e-01 -1.6531e-01 -2.1139e-02\n",
      "  6.1458e-02 -1.0773e-01]\n",
      "[ 0.044751  0.22425   0.74523  -0.3927    0.73664  -0.086284 -0.010413\n",
      "  0.41628   0.5249   -0.16484   0.29205  -0.095553  0.61566   1.602\n",
      "  0.4641    1.2693   -1.1416    0.88237   0.47702  -0.78944   1.5937\n",
      " -0.34953   0.42561   0.56537  -0.32054  -0.5776   -0.39622   1.129\n",
      "  1.8284   -0.10004   0.82675   0.46756  -0.23037   0.80698   0.52753\n",
      "  1.0272    0.21814   1.1406    1.4022    0.30931   0.57774   0.2999\n",
      " -0.21302  -0.046468  0.076242  0.91611   0.47489  -0.55653  -0.45134\n",
      "  0.049276]\n",
      "0.21404315531253815\n",
      "[ 5.0625e-01 -7.5861e-02  3.5667e-01 -1.9916e-01  4.2828e-01 -4.9580e-01\n",
      "  1.7988e-02  5.0164e-01 -7.4663e-02  1.6919e-02 -8.8081e-01 -1.6071e+00\n",
      "  4.0707e-01  1.9289e-01 -3.0754e-01  1.1850e-01 -1.6106e-04  7.4773e-01\n",
      "  1.3391e-01 -4.6638e-01 -6.7023e-01 -3.0118e-01 -3.0377e-01  1.2345e+00\n",
      "  9.0100e-02  1.3800e-01 -3.3182e-01  1.4311e+00  9.8952e-01 -4.4896e-01\n",
      " -4.5327e-01  2.9784e-01 -5.6369e-01  8.7011e-01  1.0898e+00 -1.8810e-03\n",
      "  4.1680e-01 -3.0524e-01  1.1515e-02  6.4577e-01 -2.9024e-01  3.0183e-01\n",
      " -1.0317e+00  2.7942e-02 -5.0534e-01 -5.4613e-01  6.9852e-01 -3.7518e-01\n",
      "  6.7727e-01 -7.4941e-02]\n",
      "[ 0.94971    0.34328    0.84504   -0.88519   -0.72078   -0.29309\n",
      " -0.74678    0.65122    0.47295   -0.74011    0.1877    -0.38279\n",
      " -0.55899    0.42952   -0.26984   -0.42383   -0.31236    1.3423\n",
      " -0.78567   -0.6302     0.91819    0.21126   -0.57442    1.4549\n",
      "  0.75456   -1.6165    -0.0085015  0.0029134  0.51304   -0.47447\n",
      "  2.5306     0.85944   -0.30667    0.057765   0.66231    0.20804\n",
      "  0.64237   -0.5246    -0.053416   1.1404    -0.13703   -0.18361\n",
      "  0.45459   -0.50963   -0.025539  -0.02861    0.18048   -0.4483\n",
      "  0.40525   -0.36821  ]\n",
      "0.2701544761657715\n",
      "[ 5.0625e-01 -7.5861e-02  3.5667e-01 -1.9916e-01  4.2828e-01 -4.9580e-01\n",
      "  1.7988e-02  5.0164e-01 -7.4663e-02  1.6919e-02 -8.8081e-01 -1.6071e+00\n",
      "  4.0707e-01  1.9289e-01 -3.0754e-01  1.1850e-01 -1.6106e-04  7.4773e-01\n",
      "  1.3391e-01 -4.6638e-01 -6.7023e-01 -3.0118e-01 -3.0377e-01  1.2345e+00\n",
      "  9.0100e-02  1.3800e-01 -3.3182e-01  1.4311e+00  9.8952e-01 -4.4896e-01\n",
      " -4.5327e-01  2.9784e-01 -5.6369e-01  8.7011e-01  1.0898e+00 -1.8810e-03\n",
      "  4.1680e-01 -3.0524e-01  1.1515e-02  6.4577e-01 -2.9024e-01  3.0183e-01\n",
      " -1.0317e+00  2.7942e-02 -5.0534e-01 -5.4613e-01  6.9852e-01 -3.7518e-01\n",
      "  6.7727e-01 -7.4941e-02]\n",
      "[ 3.1973e-01 -5.3511e-01  1.6837e-01 -5.1493e-01 -5.4168e-01 -5.2323e-02\n",
      " -3.4670e-01  4.1141e-01  4.8828e-01  1.9277e-01 -4.3457e-01 -5.4993e-02\n",
      " -7.0818e-01 -2.1076e-01  3.0517e-01  6.3560e-02  3.5297e-01 -1.2412e-01\n",
      " -1.2177e-01 -4.1381e-01  7.4801e-01  2.1710e-01  7.7925e-03  5.0231e-01\n",
      "  2.4590e-01 -2.2516e+00  5.8344e-01  1.2077e-01  1.2825e+00 -1.0129e+00\n",
      "  2.9238e+00  7.5870e-01 -1.0051e+00 -1.7133e-01 -6.1737e-01 -1.5465e-02\n",
      "  3.0681e-01 -7.0980e-01 -3.9136e-01  5.3807e-01 -1.5420e-01  8.3077e-02\n",
      " -2.0523e-03 -1.1419e-01  6.6978e-01 -2.2367e-01 -1.6531e-01 -2.1139e-02\n",
      "  6.1458e-02 -1.0773e-01]\n",
      "0.05217964947223663\n",
      "[ 5.0625e-01 -7.5861e-02  3.5667e-01 -1.9916e-01  4.2828e-01 -4.9580e-01\n",
      "  1.7988e-02  5.0164e-01 -7.4663e-02  1.6919e-02 -8.8081e-01 -1.6071e+00\n",
      "  4.0707e-01  1.9289e-01 -3.0754e-01  1.1850e-01 -1.6106e-04  7.4773e-01\n",
      "  1.3391e-01 -4.6638e-01 -6.7023e-01 -3.0118e-01 -3.0377e-01  1.2345e+00\n",
      "  9.0100e-02  1.3800e-01 -3.3182e-01  1.4311e+00  9.8952e-01 -4.4896e-01\n",
      " -4.5327e-01  2.9784e-01 -5.6369e-01  8.7011e-01  1.0898e+00 -1.8810e-03\n",
      "  4.1680e-01 -3.0524e-01  1.1515e-02  6.4577e-01 -2.9024e-01  3.0183e-01\n",
      " -1.0317e+00  2.7942e-02 -5.0534e-01 -5.4613e-01  6.9852e-01 -3.7518e-01\n",
      "  6.7727e-01 -7.4941e-02]\n",
      "[ 5.0625e-01 -7.5861e-02  3.5667e-01 -1.9916e-01  4.2828e-01 -4.9580e-01\n",
      "  1.7988e-02  5.0164e-01 -7.4663e-02  1.6919e-02 -8.8081e-01 -1.6071e+00\n",
      "  4.0707e-01  1.9289e-01 -3.0754e-01  1.1850e-01 -1.6106e-04  7.4773e-01\n",
      "  1.3391e-01 -4.6638e-01 -6.7023e-01 -3.0118e-01 -3.0377e-01  1.2345e+00\n",
      "  9.0100e-02  1.3800e-01 -3.3182e-01  1.4311e+00  9.8952e-01 -4.4896e-01\n",
      " -4.5327e-01  2.9784e-01 -5.6369e-01  8.7011e-01  1.0898e+00 -1.8810e-03\n",
      "  4.1680e-01 -3.0524e-01  1.1515e-02  6.4577e-01 -2.9024e-01  3.0183e-01\n",
      " -1.0317e+00  2.7942e-02 -5.0534e-01 -5.4613e-01  6.9852e-01 -3.7518e-01\n",
      "  6.7727e-01 -7.4941e-02]\n",
      "1.0\n",
      "[ 5.0625e-01 -7.5861e-02  3.5667e-01 -1.9916e-01  4.2828e-01 -4.9580e-01\n",
      "  1.7988e-02  5.0164e-01 -7.4663e-02  1.6919e-02 -8.8081e-01 -1.6071e+00\n",
      "  4.0707e-01  1.9289e-01 -3.0754e-01  1.1850e-01 -1.6106e-04  7.4773e-01\n",
      "  1.3391e-01 -4.6638e-01 -6.7023e-01 -3.0118e-01 -3.0377e-01  1.2345e+00\n",
      "  9.0100e-02  1.3800e-01 -3.3182e-01  1.4311e+00  9.8952e-01 -4.4896e-01\n",
      " -4.5327e-01  2.9784e-01 -5.6369e-01  8.7011e-01  1.0898e+00 -1.8810e-03\n",
      "  4.1680e-01 -3.0524e-01  1.1515e-02  6.4577e-01 -2.9024e-01  3.0183e-01\n",
      " -1.0317e+00  2.7942e-02 -5.0534e-01 -5.4613e-01  6.9852e-01 -3.7518e-01\n",
      "  6.7727e-01 -7.4941e-02]\n",
      "[-2.7866e-01 -2.9872e-01  6.6275e-01 -9.5487e-02  5.2230e-01 -4.3949e-01\n",
      "  3.1018e-02 -2.6427e-01  3.3310e-01 -2.1021e-01  1.0597e-01 -5.5006e-01\n",
      " -2.3407e-01  1.0332e+00  1.7066e-01  7.3381e-01 -5.9416e-01 -1.6251e-01\n",
      " -8.4043e-01 -7.2089e-01  6.8816e-01 -2.0450e-01 -7.5548e-01 -1.1226e-01\n",
      "  9.6886e-01 -6.6068e-01 -8.9874e-02 -1.0576e-01  8.8008e-01 -3.2770e-01\n",
      "  2.1922e+00 -4.8859e-02 -4.8141e-01 -1.6170e-01 -2.0312e-01  2.7131e-01\n",
      "  1.1705e+00  7.6043e-05  2.2993e-01  7.0042e-01 -1.6987e-01 -5.4231e-02\n",
      " -9.3794e-01  8.7418e-01 -1.9089e-01  3.4681e-01  1.2777e+00 -4.2743e-01\n",
      " -3.2283e-01 -2.4891e-01]\n",
      "0.20132066309452057\n",
      "[ 5.0625e-01 -7.5861e-02  3.5667e-01 -1.9916e-01  4.2828e-01 -4.9580e-01\n",
      "  1.7988e-02  5.0164e-01 -7.4663e-02  1.6919e-02 -8.8081e-01 -1.6071e+00\n",
      "  4.0707e-01  1.9289e-01 -3.0754e-01  1.1850e-01 -1.6106e-04  7.4773e-01\n",
      "  1.3391e-01 -4.6638e-01 -6.7023e-01 -3.0118e-01 -3.0377e-01  1.2345e+00\n",
      "  9.0100e-02  1.3800e-01 -3.3182e-01  1.4311e+00  9.8952e-01 -4.4896e-01\n",
      " -4.5327e-01  2.9784e-01 -5.6369e-01  8.7011e-01  1.0898e+00 -1.8810e-03\n",
      "  4.1680e-01 -3.0524e-01  1.1515e-02  6.4577e-01 -2.9024e-01  3.0183e-01\n",
      " -1.0317e+00  2.7942e-02 -5.0534e-01 -5.4613e-01  6.9852e-01 -3.7518e-01\n",
      "  6.7727e-01 -7.4941e-02]\n",
      "[ 0.044751  0.22425   0.74523  -0.3927    0.73664  -0.086284 -0.010413\n",
      "  0.41628   0.5249   -0.16484   0.29205  -0.095553  0.61566   1.602\n",
      "  0.4641    1.2693   -1.1416    0.88237   0.47702  -0.78944   1.5937\n",
      " -0.34953   0.42561   0.56537  -0.32054  -0.5776   -0.39622   1.129\n",
      "  1.8284   -0.10004   0.82675   0.46756  -0.23037   0.80698   0.52753\n",
      "  1.0272    0.21814   1.1406    1.4022    0.30931   0.57774   0.2999\n",
      " -0.21302  -0.046468  0.076242  0.91611   0.47489  -0.55653  -0.45134\n",
      "  0.049276]\n",
      "0.3044391870498657\n",
      "[-2.7866e-01 -2.9872e-01  6.6275e-01 -9.5487e-02  5.2230e-01 -4.3949e-01\n",
      "  3.1018e-02 -2.6427e-01  3.3310e-01 -2.1021e-01  1.0597e-01 -5.5006e-01\n",
      " -2.3407e-01  1.0332e+00  1.7066e-01  7.3381e-01 -5.9416e-01 -1.6251e-01\n",
      " -8.4043e-01 -7.2089e-01  6.8816e-01 -2.0450e-01 -7.5548e-01 -1.1226e-01\n",
      "  9.6886e-01 -6.6068e-01 -8.9874e-02 -1.0576e-01  8.8008e-01 -3.2770e-01\n",
      "  2.1922e+00 -4.8859e-02 -4.8141e-01 -1.6170e-01 -2.0312e-01  2.7131e-01\n",
      "  1.1705e+00  7.6043e-05  2.2993e-01  7.0042e-01 -1.6987e-01 -5.4231e-02\n",
      " -9.3794e-01  8.7418e-01 -1.9089e-01  3.4681e-01  1.2777e+00 -4.2743e-01\n",
      " -3.2283e-01 -2.4891e-01]\n",
      "[ 0.94971    0.34328    0.84504   -0.88519   -0.72078   -0.29309\n",
      " -0.74678    0.65122    0.47295   -0.74011    0.1877    -0.38279\n",
      " -0.55899    0.42952   -0.26984   -0.42383   -0.31236    1.3423\n",
      " -0.78567   -0.6302     0.91819    0.21126   -0.57442    1.4549\n",
      "  0.75456   -1.6165    -0.0085015  0.0029134  0.51304   -0.47447\n",
      "  2.5306     0.85944   -0.30667    0.057765   0.66231    0.20804\n",
      "  0.64237   -0.5246    -0.053416   1.1404    -0.13703   -0.18361\n",
      "  0.45459   -0.50963   -0.025539  -0.02861    0.18048   -0.4483\n",
      "  0.40525   -0.36821  ]\n",
      "0.5137549638748169\n",
      "[-2.7866e-01 -2.9872e-01  6.6275e-01 -9.5487e-02  5.2230e-01 -4.3949e-01\n",
      "  3.1018e-02 -2.6427e-01  3.3310e-01 -2.1021e-01  1.0597e-01 -5.5006e-01\n",
      " -2.3407e-01  1.0332e+00  1.7066e-01  7.3381e-01 -5.9416e-01 -1.6251e-01\n",
      " -8.4043e-01 -7.2089e-01  6.8816e-01 -2.0450e-01 -7.5548e-01 -1.1226e-01\n",
      "  9.6886e-01 -6.6068e-01 -8.9874e-02 -1.0576e-01  8.8008e-01 -3.2770e-01\n",
      "  2.1922e+00 -4.8859e-02 -4.8141e-01 -1.6170e-01 -2.0312e-01  2.7131e-01\n",
      "  1.1705e+00  7.6043e-05  2.2993e-01  7.0042e-01 -1.6987e-01 -5.4231e-02\n",
      " -9.3794e-01  8.7418e-01 -1.9089e-01  3.4681e-01  1.2777e+00 -4.2743e-01\n",
      " -3.2283e-01 -2.4891e-01]\n",
      "[ 3.1973e-01 -5.3511e-01  1.6837e-01 -5.1493e-01 -5.4168e-01 -5.2323e-02\n",
      " -3.4670e-01  4.1141e-01  4.8828e-01  1.9277e-01 -4.3457e-01 -5.4993e-02\n",
      " -7.0818e-01 -2.1076e-01  3.0517e-01  6.3560e-02  3.5297e-01 -1.2412e-01\n",
      " -1.2177e-01 -4.1381e-01  7.4801e-01  2.1710e-01  7.7925e-03  5.0231e-01\n",
      "  2.4590e-01 -2.2516e+00  5.8344e-01  1.2077e-01  1.2825e+00 -1.0129e+00\n",
      "  2.9238e+00  7.5870e-01 -1.0051e+00 -1.7133e-01 -6.1737e-01 -1.5465e-02\n",
      "  3.0681e-01 -7.0980e-01 -3.9136e-01  5.3807e-01 -1.5420e-01  8.3077e-02\n",
      " -2.0523e-03 -1.1419e-01  6.6978e-01 -2.2367e-01 -1.6531e-01 -2.1139e-02\n",
      "  6.1458e-02 -1.0773e-01]\n",
      "0.5098719000816345\n",
      "[-2.7866e-01 -2.9872e-01  6.6275e-01 -9.5487e-02  5.2230e-01 -4.3949e-01\n",
      "  3.1018e-02 -2.6427e-01  3.3310e-01 -2.1021e-01  1.0597e-01 -5.5006e-01\n",
      " -2.3407e-01  1.0332e+00  1.7066e-01  7.3381e-01 -5.9416e-01 -1.6251e-01\n",
      " -8.4043e-01 -7.2089e-01  6.8816e-01 -2.0450e-01 -7.5548e-01 -1.1226e-01\n",
      "  9.6886e-01 -6.6068e-01 -8.9874e-02 -1.0576e-01  8.8008e-01 -3.2770e-01\n",
      "  2.1922e+00 -4.8859e-02 -4.8141e-01 -1.6170e-01 -2.0312e-01  2.7131e-01\n",
      "  1.1705e+00  7.6043e-05  2.2993e-01  7.0042e-01 -1.6987e-01 -5.4231e-02\n",
      " -9.3794e-01  8.7418e-01 -1.9089e-01  3.4681e-01  1.2777e+00 -4.2743e-01\n",
      " -3.2283e-01 -2.4891e-01]\n",
      "[ 5.0625e-01 -7.5861e-02  3.5667e-01 -1.9916e-01  4.2828e-01 -4.9580e-01\n",
      "  1.7988e-02  5.0164e-01 -7.4663e-02  1.6919e-02 -8.8081e-01 -1.6071e+00\n",
      "  4.0707e-01  1.9289e-01 -3.0754e-01  1.1850e-01 -1.6106e-04  7.4773e-01\n",
      "  1.3391e-01 -4.6638e-01 -6.7023e-01 -3.0118e-01 -3.0377e-01  1.2345e+00\n",
      "  9.0100e-02  1.3800e-01 -3.3182e-01  1.4311e+00  9.8952e-01 -4.4896e-01\n",
      " -4.5327e-01  2.9784e-01 -5.6369e-01  8.7011e-01  1.0898e+00 -1.8810e-03\n",
      "  4.1680e-01 -3.0524e-01  1.1515e-02  6.4577e-01 -2.9024e-01  3.0183e-01\n",
      " -1.0317e+00  2.7942e-02 -5.0534e-01 -5.4613e-01  6.9852e-01 -3.7518e-01\n",
      "  6.7727e-01 -7.4941e-02]\n",
      "0.20132066309452057\n",
      "[-2.7866e-01 -2.9872e-01  6.6275e-01 -9.5487e-02  5.2230e-01 -4.3949e-01\n",
      "  3.1018e-02 -2.6427e-01  3.3310e-01 -2.1021e-01  1.0597e-01 -5.5006e-01\n",
      " -2.3407e-01  1.0332e+00  1.7066e-01  7.3381e-01 -5.9416e-01 -1.6251e-01\n",
      " -8.4043e-01 -7.2089e-01  6.8816e-01 -2.0450e-01 -7.5548e-01 -1.1226e-01\n",
      "  9.6886e-01 -6.6068e-01 -8.9874e-02 -1.0576e-01  8.8008e-01 -3.2770e-01\n",
      "  2.1922e+00 -4.8859e-02 -4.8141e-01 -1.6170e-01 -2.0312e-01  2.7131e-01\n",
      "  1.1705e+00  7.6043e-05  2.2993e-01  7.0042e-01 -1.6987e-01 -5.4231e-02\n",
      " -9.3794e-01  8.7418e-01 -1.9089e-01  3.4681e-01  1.2777e+00 -4.2743e-01\n",
      " -3.2283e-01 -2.4891e-01]\n",
      "[-2.7866e-01 -2.9872e-01  6.6275e-01 -9.5487e-02  5.2230e-01 -4.3949e-01\n",
      "  3.1018e-02 -2.6427e-01  3.3310e-01 -2.1021e-01  1.0597e-01 -5.5006e-01\n",
      " -2.3407e-01  1.0332e+00  1.7066e-01  7.3381e-01 -5.9416e-01 -1.6251e-01\n",
      " -8.4043e-01 -7.2089e-01  6.8816e-01 -2.0450e-01 -7.5548e-01 -1.1226e-01\n",
      "  9.6886e-01 -6.6068e-01 -8.9874e-02 -1.0576e-01  8.8008e-01 -3.2770e-01\n",
      "  2.1922e+00 -4.8859e-02 -4.8141e-01 -1.6170e-01 -2.0312e-01  2.7131e-01\n",
      "  1.1705e+00  7.6043e-05  2.2993e-01  7.0042e-01 -1.6987e-01 -5.4231e-02\n",
      " -9.3794e-01  8.7418e-01 -1.9089e-01  3.4681e-01  1.2777e+00 -4.2743e-01\n",
      " -3.2283e-01 -2.4891e-01]\n",
      "1.0\n",
      "[-2.7866e-01 -2.9872e-01  6.6275e-01 -9.5487e-02  5.2230e-01 -4.3949e-01\n",
      "  3.1018e-02 -2.6427e-01  3.3310e-01 -2.1021e-01  1.0597e-01 -5.5006e-01\n",
      " -2.3407e-01  1.0332e+00  1.7066e-01  7.3381e-01 -5.9416e-01 -1.6251e-01\n",
      " -8.4043e-01 -7.2089e-01  6.8816e-01 -2.0450e-01 -7.5548e-01 -1.1226e-01\n",
      "  9.6886e-01 -6.6068e-01 -8.9874e-02 -1.0576e-01  8.8008e-01 -3.2770e-01\n",
      "  2.1922e+00 -4.8859e-02 -4.8141e-01 -1.6170e-01 -2.0312e-01  2.7131e-01\n",
      "  1.1705e+00  7.6043e-05  2.2993e-01  7.0042e-01 -1.6987e-01 -5.4231e-02\n",
      " -9.3794e-01  8.7418e-01 -1.9089e-01  3.4681e-01  1.2777e+00 -4.2743e-01\n",
      " -3.2283e-01 -2.4891e-01]\n",
      "[ 0.044751  0.22425   0.74523  -0.3927    0.73664  -0.086284 -0.010413\n",
      "  0.41628   0.5249   -0.16484   0.29205  -0.095553  0.61566   1.602\n",
      "  0.4641    1.2693   -1.1416    0.88237   0.47702  -0.78944   1.5937\n",
      " -0.34953   0.42561   0.56537  -0.32054  -0.5776   -0.39622   1.129\n",
      "  1.8284   -0.10004   0.82675   0.46756  -0.23037   0.80698   0.52753\n",
      "  1.0272    0.21814   1.1406    1.4022    0.30931   0.57774   0.2999\n",
      " -0.21302  -0.046468  0.076242  0.91611   0.47489  -0.55653  -0.45134\n",
      "  0.049276]\n",
      "0.47796374559402466\n",
      "[ 0.044751  0.22425   0.74523  -0.3927    0.73664  -0.086284 -0.010413\n",
      "  0.41628   0.5249   -0.16484   0.29205  -0.095553  0.61566   1.602\n",
      "  0.4641    1.2693   -1.1416    0.88237   0.47702  -0.78944   1.5937\n",
      " -0.34953   0.42561   0.56537  -0.32054  -0.5776   -0.39622   1.129\n",
      "  1.8284   -0.10004   0.82675   0.46756  -0.23037   0.80698   0.52753\n",
      "  1.0272    0.21814   1.1406    1.4022    0.30931   0.57774   0.2999\n",
      " -0.21302  -0.046468  0.076242  0.91611   0.47489  -0.55653  -0.45134\n",
      "  0.049276]\n",
      "[ 0.94971    0.34328    0.84504   -0.88519   -0.72078   -0.29309\n",
      " -0.74678    0.65122    0.47295   -0.74011    0.1877    -0.38279\n",
      " -0.55899    0.42952   -0.26984   -0.42383   -0.31236    1.3423\n",
      " -0.78567   -0.6302     0.91819    0.21126   -0.57442    1.4549\n",
      "  0.75456   -1.6165    -0.0085015  0.0029134  0.51304   -0.47447\n",
      "  2.5306     0.85944   -0.30667    0.057765   0.66231    0.20804\n",
      "  0.64237   -0.5246    -0.053416   1.1404    -0.13703   -0.18361\n",
      "  0.45459   -0.50963   -0.025539  -0.02861    0.18048   -0.4483\n",
      "  0.40525   -0.36821  ]\n",
      "0.3487786650657654\n",
      "[ 0.044751  0.22425   0.74523  -0.3927    0.73664  -0.086284 -0.010413\n",
      "  0.41628   0.5249   -0.16484   0.29205  -0.095553  0.61566   1.602\n",
      "  0.4641    1.2693   -1.1416    0.88237   0.47702  -0.78944   1.5937\n",
      " -0.34953   0.42561   0.56537  -0.32054  -0.5776   -0.39622   1.129\n",
      "  1.8284   -0.10004   0.82675   0.46756  -0.23037   0.80698   0.52753\n",
      "  1.0272    0.21814   1.1406    1.4022    0.30931   0.57774   0.2999\n",
      " -0.21302  -0.046468  0.076242  0.91611   0.47489  -0.55653  -0.45134\n",
      "  0.049276]\n",
      "[ 3.1973e-01 -5.3511e-01  1.6837e-01 -5.1493e-01 -5.4168e-01 -5.2323e-02\n",
      " -3.4670e-01  4.1141e-01  4.8828e-01  1.9277e-01 -4.3457e-01 -5.4993e-02\n",
      " -7.0818e-01 -2.1076e-01  3.0517e-01  6.3560e-02  3.5297e-01 -1.2412e-01\n",
      " -1.2177e-01 -4.1381e-01  7.4801e-01  2.1710e-01  7.7925e-03  5.0231e-01\n",
      "  2.4590e-01 -2.2516e+00  5.8344e-01  1.2077e-01  1.2825e+00 -1.0129e+00\n",
      "  2.9238e+00  7.5870e-01 -1.0051e+00 -1.7133e-01 -6.1737e-01 -1.5465e-02\n",
      "  3.0681e-01 -7.0980e-01 -3.9136e-01  5.3807e-01 -1.5420e-01  8.3077e-02\n",
      " -2.0523e-03 -1.1419e-01  6.6978e-01 -2.2367e-01 -1.6531e-01 -2.1139e-02\n",
      "  6.1458e-02 -1.0773e-01]\n",
      "0.21404315531253815\n",
      "[ 0.044751  0.22425   0.74523  -0.3927    0.73664  -0.086284 -0.010413\n",
      "  0.41628   0.5249   -0.16484   0.29205  -0.095553  0.61566   1.602\n",
      "  0.4641    1.2693   -1.1416    0.88237   0.47702  -0.78944   1.5937\n",
      " -0.34953   0.42561   0.56537  -0.32054  -0.5776   -0.39622   1.129\n",
      "  1.8284   -0.10004   0.82675   0.46756  -0.23037   0.80698   0.52753\n",
      "  1.0272    0.21814   1.1406    1.4022    0.30931   0.57774   0.2999\n",
      " -0.21302  -0.046468  0.076242  0.91611   0.47489  -0.55653  -0.45134\n",
      "  0.049276]\n",
      "[ 5.0625e-01 -7.5861e-02  3.5667e-01 -1.9916e-01  4.2828e-01 -4.9580e-01\n",
      "  1.7988e-02  5.0164e-01 -7.4663e-02  1.6919e-02 -8.8081e-01 -1.6071e+00\n",
      "  4.0707e-01  1.9289e-01 -3.0754e-01  1.1850e-01 -1.6106e-04  7.4773e-01\n",
      "  1.3391e-01 -4.6638e-01 -6.7023e-01 -3.0118e-01 -3.0377e-01  1.2345e+00\n",
      "  9.0100e-02  1.3800e-01 -3.3182e-01  1.4311e+00  9.8952e-01 -4.4896e-01\n",
      " -4.5327e-01  2.9784e-01 -5.6369e-01  8.7011e-01  1.0898e+00 -1.8810e-03\n",
      "  4.1680e-01 -3.0524e-01  1.1515e-02  6.4577e-01 -2.9024e-01  3.0183e-01\n",
      " -1.0317e+00  2.7942e-02 -5.0534e-01 -5.4613e-01  6.9852e-01 -3.7518e-01\n",
      "  6.7727e-01 -7.4941e-02]\n",
      "0.3044391870498657\n",
      "[ 0.044751  0.22425   0.74523  -0.3927    0.73664  -0.086284 -0.010413\n",
      "  0.41628   0.5249   -0.16484   0.29205  -0.095553  0.61566   1.602\n",
      "  0.4641    1.2693   -1.1416    0.88237   0.47702  -0.78944   1.5937\n",
      " -0.34953   0.42561   0.56537  -0.32054  -0.5776   -0.39622   1.129\n",
      "  1.8284   -0.10004   0.82675   0.46756  -0.23037   0.80698   0.52753\n",
      "  1.0272    0.21814   1.1406    1.4022    0.30931   0.57774   0.2999\n",
      " -0.21302  -0.046468  0.076242  0.91611   0.47489  -0.55653  -0.45134\n",
      "  0.049276]\n",
      "[-2.7866e-01 -2.9872e-01  6.6275e-01 -9.5487e-02  5.2230e-01 -4.3949e-01\n",
      "  3.1018e-02 -2.6427e-01  3.3310e-01 -2.1021e-01  1.0597e-01 -5.5006e-01\n",
      " -2.3407e-01  1.0332e+00  1.7066e-01  7.3381e-01 -5.9416e-01 -1.6251e-01\n",
      " -8.4043e-01 -7.2089e-01  6.8816e-01 -2.0450e-01 -7.5548e-01 -1.1226e-01\n",
      "  9.6886e-01 -6.6068e-01 -8.9874e-02 -1.0576e-01  8.8008e-01 -3.2770e-01\n",
      "  2.1922e+00 -4.8859e-02 -4.8141e-01 -1.6170e-01 -2.0312e-01  2.7131e-01\n",
      "  1.1705e+00  7.6043e-05  2.2993e-01  7.0042e-01 -1.6987e-01 -5.4231e-02\n",
      " -9.3794e-01  8.7418e-01 -1.9089e-01  3.4681e-01  1.2777e+00 -4.2743e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -3.2283e-01 -2.4891e-01]\n",
      "0.47796374559402466\n",
      "[ 0.044751  0.22425   0.74523  -0.3927    0.73664  -0.086284 -0.010413\n",
      "  0.41628   0.5249   -0.16484   0.29205  -0.095553  0.61566   1.602\n",
      "  0.4641    1.2693   -1.1416    0.88237   0.47702  -0.78944   1.5937\n",
      " -0.34953   0.42561   0.56537  -0.32054  -0.5776   -0.39622   1.129\n",
      "  1.8284   -0.10004   0.82675   0.46756  -0.23037   0.80698   0.52753\n",
      "  1.0272    0.21814   1.1406    1.4022    0.30931   0.57774   0.2999\n",
      " -0.21302  -0.046468  0.076242  0.91611   0.47489  -0.55653  -0.45134\n",
      "  0.049276]\n",
      "[ 0.044751  0.22425   0.74523  -0.3927    0.73664  -0.086284 -0.010413\n",
      "  0.41628   0.5249   -0.16484   0.29205  -0.095553  0.61566   1.602\n",
      "  0.4641    1.2693   -1.1416    0.88237   0.47702  -0.78944   1.5937\n",
      " -0.34953   0.42561   0.56537  -0.32054  -0.5776   -0.39622   1.129\n",
      "  1.8284   -0.10004   0.82675   0.46756  -0.23037   0.80698   0.52753\n",
      "  1.0272    0.21814   1.1406    1.4022    0.30931   0.57774   0.2999\n",
      " -0.21302  -0.046468  0.076242  0.91611   0.47489  -0.55653  -0.45134\n",
      "  0.049276]\n",
      "1.0\n",
      "[[0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "#Labels from similar image b, with a refridgerator\n",
    "test_labels = [glove[\"train\"], glove[\"stop\"], glove[\"stoplight\"], glove[\"clock\"],glove[\"refrigerator\"]]\n",
    "\n",
    "#Now let's see which one is the furthest from the others\n",
    "empty_list =np.array([[0 for index in range(len(test_labels))] for index in range(len(test_labels))]).reshape(len(test_labels),-1)\n",
    "\n",
    "for index in range(len(test_labels)):\n",
    "    for sec_ind in range(len(test_labels)):\n",
    "        print(test_labels[index])\n",
    "        print(test_labels[sec_ind])\n",
    "        print(1 - spatial.distance.cosine(test_labels[index], test_labels[sec_ind]))\n",
    "\n",
    "print(empty_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain Generation\n",
    "\n",
    "The cells below are specifically related to generating the terms related to the specific domain, which we can generate with a list of words:\n",
    "\n",
    "```python\n",
    "def gen_domain(list_words:list, depth:int) -> list:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_domain(list_domain:list, depth:int) -> set:\n",
    "    \"\"\"\n",
    "    This function takes in a list of strings, which represents the domain, and generates the relavent list of glove embeddings that represents this domain. \n",
    "    \n",
    "    This does it through a recursive methodology\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    list_domain: list\n",
    "    List of string terms that represent the domain\n",
    "    \n",
    "    depth: int\n",
    "    How many layers should be used to generate the domain\n",
    "    \n",
    "    Returns\n",
    "    ---------\n",
    "    Set of strings of the terms that we should get the glove embeddings for\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    list_embeddings = set() #The final set representing the domain embeddings\n",
    "    \n",
    "    if depth == 0: #If we have gotten back to depth 0 it means we have added all the words to the depth we want\n",
    "        return list_embeddings\n",
    "    \n",
    "    \n",
    "    #We will use a BFS type function to generate our domain\n",
    "    queue = set(list_domain)\n",
    "    \n",
    "    for word in queue: #For each domain word\n",
    "        \n",
    "        #Add the first thing in the queue to our list\n",
    "        list_embeddings.add(word)\n",
    "        \n",
    "        #Get similar terms\n",
    "        list_terms = set(word for word, similarity in glove.similar_by_vector(glove[word]))\n",
    "        #Add its most similar words using a recursive call\n",
    "        list_embeddings.update(gen_domain(list_terms, depth - 1))\n",
    "    \n",
    "    return list_embeddings    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make a more advanced version of the above, but where we try to make sure that the terms are similar to existing values in the domain.\n",
    "\n",
    "The primary step in doing this is to define a **threshold** value that we only add values above that:\n",
    "- Through basic visual analysis, both *0.7* and *0.75* can be tried for threshold values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_thresh_domain(list_domain:list, depth:int, threshold=0.7) -> set:\n",
    "    \"\"\"\n",
    "    This function takes in a list of strings, which represents the domain, and generates the relavent list of glove embeddings that represents this domain. \n",
    "    \n",
    "    This does it through a recursive methodology and uses a threshold value to make sure that only relavent values are pulled up.\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    list_domain: list\n",
    "    List of string terms that represent the domain\n",
    "    \n",
    "    depth: int\n",
    "    How many layers should be used to generate the domain\n",
    "    \n",
    "    threshold: float\n",
    "    The similarity threshold that we will only add values above, by default = 0.7\n",
    "    \n",
    "    Returns\n",
    "    ---------\n",
    "    Set of strings of the terms that we should get the glove embeddings for\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    list_embeddings = set() #The final set representing the domain embeddings\n",
    "    \n",
    "    if depth == 0: #If we have gotten back to depth 0 it means we have added all the words to the depth we want\n",
    "        return list_embeddings\n",
    "    \n",
    "    \n",
    "    #We will use a BFS type function to generate our domain\n",
    "    queue = set(list_domain)\n",
    " \n",
    "    for word in queue: #For each domain word\n",
    "        \n",
    "        #Add the first thing in the queue to our list\n",
    "        list_embeddings.add(word)\n",
    "        \n",
    "        #Get similar terms\n",
    "        list_terms = set(word for word, similarity in glove.similar_by_vector(glove[word]) if similarity >= threshold)\n",
    "        \n",
    "        #Add its most similar words using a recursive call\n",
    "        list_embeddings.update(gen_thresh_domain(list_terms, depth - 1))\n",
    "    \n",
    "    return list_embeddings    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_embeddings(domain:set) -> list:\n",
    "    \"\"\"\n",
    "    Converts the domain of terms to a list of related embeddings\n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    domain: set\n",
    "    The set of terms that define the domain, each term is a string\n",
    "    \n",
    "    Returns\n",
    "    ---------\n",
    "    list of glove embeddings\n",
    "    \"\"\"\n",
    "    return [glove[word] for word in domain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = gen_thresh_domain([\"stop\",\"stoplight\"], 2)\n",
    "len(b)\n",
    "\n",
    "#print(set.intersection(vals, vally))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a test with this **huge** domain and see how close the vectors we had above are (the three vectors in the section above)\n",
    "\n",
    "We need to make sure that we **convert our set of terms to list of glove embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doneA\n",
      "Dist Similar A to Domain: with Depth 4\n",
      "1106.8966\n",
      "\n",
      "\n",
      "Dist Similar B to Domain: with Depth 4\n",
      "1075.3406\n",
      "\n",
      "\n",
      "Dist Different A to Domain: with Depth 4\n",
      "1343.2302\n",
      "_____________________________\n",
      "Sim A, Sim B  = 31.5560302734375\n",
      "Sim A, Diff A= 236.3336181640625\n",
      "Sim B, Diff A  = 267.8896484375\n"
     ]
    }
   ],
   "source": [
    "depth = 4\n",
    "bb = convert_to_embeddings(gen_thresh_domain([\"stop\",\"stoplight\"], depth))\n",
    "print(\"doneA\")\n",
    "cc = convert_to_embeddings(gen_thresh_domain([\"apple\",\"banana\"], depth))\n",
    "# print(\"Distances between similar_image_a, domain: \")\n",
    "# print(calcuate_distances(similar_image_a,bb))\n",
    "\n",
    "# print(\"\\n\")\n",
    "# print(\"Distances between different_image_a, domain: \")\n",
    "# print(calcuate_distances(different_image_a,bb))\n",
    "\n",
    "# print(\"\\n\")\n",
    "# print(\"Distances between similar_image_b, domain: \")\n",
    "# print(calcuate_distances(similar_image_b,bb))\n",
    "\n",
    "# print(\"\\n\")\n",
    "# print(\"Printing the sums of each:\")\n",
    "# print(\"\\n\")\n",
    "print(\"Dist Similar A to Domain: with Depth {}\".format(depth))\n",
    "a = calcuate_distance(similar_image_a,bb)\n",
    "print(a)\n",
    "print(\"\\n\")\n",
    "print(\"Dist Similar B to Domain: with Depth {}\".format(depth))\n",
    "b = calcuate_distance(similar_image_b,bb)\n",
    "print(b)\n",
    "print(\"\\n\")\n",
    "print(\"Dist Different A to Domain: with Depth {}\".format(depth))\n",
    "c = calcuate_distance(different_image_a,bb)\n",
    "print(c)\n",
    "\n",
    "print(\"_____________________________\")\n",
    "print(\"Sim A, Sim B  = {}\".format(abs(b-a)))\n",
    "print(\"Sim A, Diff A= {}\".format(abs(c-a)))\n",
    "print(\"Sim B, Diff A  = {}\".format(abs(c-b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Vectors from Captions\n",
    "\n",
    "To generate vectors from captions, there are several different methods, but I think the most basic way to do it for now is to make use of **stop-words**. \n",
    "\n",
    "## Stop Words\n",
    "\n",
    "These are words in the english language like \"a\" \"an\" \"the\" and other terms we may consider to be *filler terms*. The idea is that by stripping these words, we can create more comprehensive and representative vectors. The `list_of_stop_words` below is from [this link](https://gist.github.com/sebleier/554280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_stop_words = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def gen_cap_terms(caption:str) -> list:\n",
    "    \"\"\"\n",
    "    This function takes in a caption as a string and will return a list of the most important words that are also found in Glove\n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    caption : str\n",
    "        The caption in a string form\n",
    "    \n",
    "    Returns\n",
    "    ---------\n",
    "    set[str]\n",
    "        Returns a set of important terms recognized by Glove as a String\n",
    "    \"\"\"\n",
    "    caption = \"\".join(token for token in caption if token not in string.punctuation)\n",
    "\n",
    "    caption_list = caption.split() #Tokenizes the words\n",
    "    \n",
    "    ret_set = set()  #set of terms we will return\n",
    "    \n",
    "    for word in caption_list:\n",
    "        if word not in list_of_stop_words and word in glove:\n",
    "            ret_set.add(word)\n",
    "    \n",
    "    return ret_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bunch', 'hanging', 'bananas'}\n"
     ]
    }
   ],
   "source": [
    "#All test are from one image Sim Image A\n",
    "test = \"a pole with both street lights and a stop sign.\"\n",
    "test2 = \"a stop sign, road sign and traffic light sitting near buildings.\"\n",
    "#Trying with stuff from diff image (Diff Image A)\n",
    "diff1 = \"a bunch of bananas are hanging.\"\n",
    "diff2 = \"a store with lots of unripe bananas and other products.\"\n",
    "print(gen_cap_terms(diff1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"car.\" in glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a way to get caption related info, let's run this with captions and see what distances between **same caption and image** are and **different captions and image** as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dist Similar A to Caption A = 19.996829986572266\n",
      "\n",
      "\n",
      "Dist Similar A to Caption B = 27.7014102935791\n",
      "\n",
      "\n",
      "Dist Similar A to Diff Cap A = 15.85655403137207\n",
      "\n",
      "\n",
      "Dist Diff Image A to Diff Cap A = 22.23456382751465\n",
      "\n",
      "\n",
      "Dist Diff Image A to Diff Cap B = 15.85655403137207\n",
      "\n",
      "\n",
      "Dist Diff Image A to Sim Cap A = 26.68285369873047\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cap_a = convert_to_embeddings(gen_cap_terms(test))\n",
    "a = calcuate_distance(similar_image_a,cap_a)\n",
    "print(\"Dist Similar A to Caption A = {}\".format(a))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "cap_b = convert_to_embeddings(gen_cap_terms(test2))\n",
    "b = calcuate_distance(similar_image_a,cap_b)\n",
    "print(\"Dist Similar A to Caption B = {}\".format(b))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "cap_c = convert_to_embeddings(gen_cap_terms(diff1))\n",
    "c = calcuate_distance(similar_image_a,cap_c)\n",
    "print(\"Dist Similar A to Diff Cap A = {}\".format(c))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "cap_d = convert_to_embeddings(gen_cap_terms(diff1))\n",
    "d = calcuate_distance(different_image_a,cap_d)\n",
    "print(\"Dist Diff Image A to Diff Cap A = {}\".format(d))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "cap_e= convert_to_embeddings(gen_cap_terms(diff2))\n",
    "e = calcuate_distance(different_image_a,cap_e)\n",
    "print(\"Dist Diff Image A to Diff Cap B = {}\".format(c))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "cap_f = convert_to_embeddings(gen_cap_terms(test))\n",
    "f = calcuate_distance(different_image_a,cap_f)\n",
    "print(\"Dist Diff Image A to Sim Cap A = {}\".format(f))\n",
    "\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'near', 'stop', 'buildings', 'traffic', 'road', 'light', 'sign', 'sitting'}\n",
      "{'stop', 'lights', 'pole', 'street', 'sign'}\n",
      "{'bunch', 'car', 'hanging', 'bananas'}\n",
      "{'bananas', 'unripe', 'lots', 'products', 'store'}\n",
      "1092.3867\n",
      "1063.3086\n",
      "1328.0569\n",
      "1351.4886\n",
      "--------------------\n",
      "1134.8907\n",
      "1179.7527\n",
      "1469.3591\n",
      "1615.6538\n"
     ]
    }
   ],
   "source": [
    "#Here we will do distances to domai instead, since this is already the baseline\n",
    "print(gen_cap_terms(test2))\n",
    "print(gen_cap_terms(test))\n",
    "print(gen_cap_terms(diff1))\n",
    "print(gen_cap_terms(diff2))\n",
    "\n",
    "\n",
    "\n",
    "cap_a = convert_to_embeddings(gen_cap_terms(test))\n",
    "cap_b = convert_to_embeddings(gen_cap_terms(test2))\n",
    "cap_d = convert_to_embeddings(gen_cap_terms(diff1))\n",
    "cap_e= convert_to_embeddings(gen_cap_terms(diff2))\n",
    "\n",
    "a = calcuate_distance(bb,cap_a)\n",
    "print(a)\n",
    "a = calcuate_distance(bb,cap_b)\n",
    "print(a)\n",
    "a = calcuate_distance(bb,cap_d)\n",
    "print(a)\n",
    "a = calcuate_distance(bb,cap_e)\n",
    "print(a)\n",
    "\n",
    "print(\"--------------------\")\n",
    "setty = convert_to_embeddings(set([\"sign\",\"light\",\"road\",\"traffic\"]))\n",
    "a = calcuate_distance(bb,setty)\n",
    "print(a)\n",
    "setty = convert_to_embeddings(set([\"lights\",\"pole\",\"sign\",\"street\"]))\n",
    "a = calcuate_distance(bb,setty)\n",
    "print(a)\n",
    "setty = convert_to_embeddings(set([\"bananas\",\"food\",\"bunch\"]))\n",
    "a = calcuate_distance(bb,setty)\n",
    "print(a)\n",
    "setty = convert_to_embeddings(set([\"bananas\",\"store\",\"unripe\"]))\n",
    "a = calcuate_distance(bb,setty)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verbs\n",
    "We will try the same thing with stripping all the verbs from the domain and see if we get better results for the two domains we specified. Verbs from [link](https://github.com/datmt/English-Verbs)\n",
    "\n",
    "First we have to read in the verbs from the text file and keep them as a list or set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set_verbs = set() #Set of Terms\n",
    "\n",
    "f = open(\"verbsList.txt\", \"r\")\n",
    "for x in f:\n",
    "  set_verbs.add(x[:-1])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abandon',\n",
       " 'abase',\n",
       " 'abate',\n",
       " 'abbreviate',\n",
       " 'abdicate',\n",
       " 'abduct',\n",
       " 'abet',\n",
       " 'abhor',\n",
       " 'abide',\n",
       " 'abjure',\n",
       " 'abnegate',\n",
       " 'abolish',\n",
       " 'abominate',\n",
       " 'abort',\n",
       " 'abound',\n",
       " 'abrade',\n",
       " 'abridge',\n",
       " 'abrogate',\n",
       " 'abscond',\n",
       " 'abseil',\n",
       " 'absent',\n",
       " 'absolve',\n",
       " 'absorb',\n",
       " 'abstain',\n",
       " 'abstract',\n",
       " 'abuse',\n",
       " 'abut',\n",
       " 'accede',\n",
       " 'accelerate',\n",
       " 'accent',\n",
       " 'accentuate',\n",
       " 'accept',\n",
       " 'access',\n",
       " 'accessorise',\n",
       " 'accessorize',\n",
       " 'acclaim',\n",
       " 'acclimate',\n",
       " 'acclimatise',\n",
       " 'acclimatize',\n",
       " 'accommodate',\n",
       " 'accompany',\n",
       " 'accomplish',\n",
       " 'accord',\n",
       " 'accost',\n",
       " 'account',\n",
       " 'accouter',\n",
       " 'accoutre',\n",
       " 'accredit',\n",
       " 'accrue',\n",
       " 'acculturate',\n",
       " 'accumulate',\n",
       " 'accuse',\n",
       " 'accustom',\n",
       " 'ace',\n",
       " 'ache',\n",
       " 'achieve',\n",
       " 'acidify',\n",
       " 'acknowledge',\n",
       " 'acquaint',\n",
       " 'acquiesce',\n",
       " 'acquire',\n",
       " 'acquit',\n",
       " 'act',\n",
       " 'action',\n",
       " 'activate',\n",
       " 'actualise',\n",
       " 'actualize',\n",
       " 'actuate',\n",
       " 'adapt',\n",
       " 'add',\n",
       " 'addle',\n",
       " 'address',\n",
       " 'adduce',\n",
       " 'adhere',\n",
       " 'adjoin',\n",
       " 'adjourn',\n",
       " 'adjudge',\n",
       " 'adjudicate',\n",
       " 'adjure',\n",
       " 'adjust',\n",
       " 'administer',\n",
       " 'admire',\n",
       " 'admit',\n",
       " 'admonish',\n",
       " 'adopt',\n",
       " 'adore',\n",
       " 'adorn',\n",
       " 'adsorb',\n",
       " 'adulterate',\n",
       " 'adumbrate',\n",
       " 'advance',\n",
       " 'advantage',\n",
       " 'advertise',\n",
       " 'advise',\n",
       " 'advocate',\n",
       " 'aerate',\n",
       " 'affect',\n",
       " 'affiliate',\n",
       " 'affirm',\n",
       " 'affix',\n",
       " 'afflict',\n",
       " 'afford',\n",
       " 'afforest',\n",
       " 'affront',\n",
       " 'age',\n",
       " 'agglomerate',\n",
       " 'aggravate',\n",
       " 'aggregate',\n",
       " 'agitate',\n",
       " 'agonise',\n",
       " 'agonize',\n",
       " 'agree',\n",
       " 'aid',\n",
       " 'ail',\n",
       " 'aim',\n",
       " 'air',\n",
       " 'airbrush',\n",
       " 'airdrop',\n",
       " 'airfreight',\n",
       " 'airlift',\n",
       " 'alarm',\n",
       " 'alert',\n",
       " 'alienate',\n",
       " 'alight',\n",
       " 'align',\n",
       " 'allay',\n",
       " 'allege',\n",
       " 'alleviate',\n",
       " 'allocate',\n",
       " 'allot',\n",
       " 'allow',\n",
       " 'alloy',\n",
       " 'allude',\n",
       " 'ally',\n",
       " 'alphabetise',\n",
       " 'alphabetize',\n",
       " 'alter',\n",
       " 'alternate',\n",
       " 'amalgamate',\n",
       " 'amass',\n",
       " 'amaze',\n",
       " 'amble',\n",
       " 'ambush',\n",
       " 'ameliorate',\n",
       " 'amend',\n",
       " 'amortise',\n",
       " 'amortize',\n",
       " 'amount',\n",
       " 'amplify',\n",
       " 'amputate',\n",
       " 'amuse',\n",
       " 'anaesthetise',\n",
       " 'anaesthetize',\n",
       " 'analyse',\n",
       " 'anchor',\n",
       " 'anesthetize',\n",
       " 'anger',\n",
       " 'angle',\n",
       " 'anglicise',\n",
       " 'anglicize',\n",
       " 'animate',\n",
       " 'anneal',\n",
       " 'annex',\n",
       " 'annihilate',\n",
       " 'annotate',\n",
       " 'announce',\n",
       " 'annoy',\n",
       " 'annul',\n",
       " 'anodise',\n",
       " 'anodize',\n",
       " 'anoint',\n",
       " 'anonymise',\n",
       " 'anonymize',\n",
       " 'answer',\n",
       " 'antagonise',\n",
       " 'antagonize',\n",
       " 'antedate',\n",
       " 'anthologise',\n",
       " 'anthologize',\n",
       " 'anticipate',\n",
       " 'ape',\n",
       " 'apologise',\n",
       " 'apologize',\n",
       " 'apostrophise',\n",
       " 'apostrophize',\n",
       " 'appal',\n",
       " 'appall',\n",
       " 'appeal',\n",
       " 'appear',\n",
       " 'appease',\n",
       " 'append',\n",
       " 'appertain',\n",
       " 'applaud',\n",
       " 'apply',\n",
       " 'appoint',\n",
       " 'apportion',\n",
       " 'appraise',\n",
       " 'appreciate',\n",
       " 'apprehend',\n",
       " 'apprentice',\n",
       " 'apprise',\n",
       " 'approach',\n",
       " 'appropriate',\n",
       " 'approve',\n",
       " 'approximate',\n",
       " 'aquaplane',\n",
       " 'arbitrate',\n",
       " 'arc',\n",
       " 'arch',\n",
       " 'archive',\n",
       " 'argue',\n",
       " 'arise',\n",
       " 'arm',\n",
       " 'arouse',\n",
       " 'arraign',\n",
       " 'arrange',\n",
       " 'array',\n",
       " 'arrest',\n",
       " 'arrive',\n",
       " 'arrogate',\n",
       " 'arse',\n",
       " 'art',\n",
       " 'articulate',\n",
       " 'ascend',\n",
       " 'ascertain',\n",
       " 'ascribe',\n",
       " 'ask',\n",
       " 'asphyxiate',\n",
       " 'aspirate',\n",
       " 'aspire',\n",
       " 'assail',\n",
       " 'assassinate',\n",
       " 'assault',\n",
       " 'assay',\n",
       " 'assemble',\n",
       " 'assent',\n",
       " 'assert',\n",
       " 'assess',\n",
       " 'assign',\n",
       " 'assimilate',\n",
       " 'assist',\n",
       " 'associate',\n",
       " 'assuage',\n",
       " 'assume',\n",
       " 'assure',\n",
       " 'asterisk',\n",
       " 'astonish',\n",
       " 'astound',\n",
       " 'atomise',\n",
       " 'atomize',\n",
       " 'atone',\n",
       " 'atrophy',\n",
       " 'attach',\n",
       " 'attack',\n",
       " 'attain',\n",
       " 'attempt',\n",
       " 'attend',\n",
       " 'attenuate',\n",
       " 'attest',\n",
       " 'attract',\n",
       " 'attribute',\n",
       " 'auction',\n",
       " 'audit',\n",
       " 'audition',\n",
       " 'augment',\n",
       " 'augur',\n",
       " 'authenticate',\n",
       " 'author',\n",
       " 'authorise',\n",
       " 'authorize',\n",
       " 'autograph',\n",
       " 'automate',\n",
       " 'autosave',\n",
       " 'autowind',\n",
       " 'avail',\n",
       " 'avenge',\n",
       " 'aver',\n",
       " 'average',\n",
       " 'avert',\n",
       " 'avoid',\n",
       " 'avow',\n",
       " 'await',\n",
       " 'awake',\n",
       " 'awaken',\n",
       " 'award',\n",
       " 'awe',\n",
       " 'ax',\n",
       " 'axe',\n",
       " 'baa',\n",
       " 'babble',\n",
       " 'baby',\n",
       " 'babysit',\n",
       " 'back',\n",
       " 'backcomb',\n",
       " 'backdate',\n",
       " 'backfill',\n",
       " 'backfire',\n",
       " 'backlight',\n",
       " 'backpack',\n",
       " 'backspace',\n",
       " 'backtrack',\n",
       " 'badger',\n",
       " 'baffle',\n",
       " 'bag',\n",
       " 'bail',\n",
       " 'bait',\n",
       " 'bake',\n",
       " 'balance',\n",
       " 'bale',\n",
       " 'ball',\n",
       " 'balloon',\n",
       " 'ballot',\n",
       " 'balls',\n",
       " 'bamboozle',\n",
       " 'ban',\n",
       " 'band',\n",
       " 'bandage',\n",
       " 'bandy',\n",
       " 'bang',\n",
       " 'bangs',\n",
       " 'banish',\n",
       " 'bank',\n",
       " 'bankroll',\n",
       " 'bankrupt',\n",
       " 'banter',\n",
       " 'baptise',\n",
       " 'baptize',\n",
       " 'bar',\n",
       " 'barbecue',\n",
       " 'bare',\n",
       " 'barf',\n",
       " 'bargain',\n",
       " 'barge',\n",
       " 'bark',\n",
       " 'barnstorm',\n",
       " 'barrack',\n",
       " 'barrel',\n",
       " 'barricade',\n",
       " 'barter',\n",
       " 'base',\n",
       " 'bash',\n",
       " 'bask',\n",
       " 'bastardise',\n",
       " 'bastardize',\n",
       " 'baste',\n",
       " 'bat',\n",
       " 'batch',\n",
       " 'bath',\n",
       " 'bathe',\n",
       " 'batten',\n",
       " 'batter',\n",
       " 'battle',\n",
       " 'baulk',\n",
       " 'bawl',\n",
       " 'bay',\n",
       " 'bayonet',\n",
       " 'be',\n",
       " 'beach',\n",
       " 'beam',\n",
       " 'bean',\n",
       " 'bear',\n",
       " 'beard',\n",
       " 'beat',\n",
       " 'beatbox',\n",
       " 'beatboxer',\n",
       " 'beatify',\n",
       " 'beautify',\n",
       " 'beaver',\n",
       " 'beckon',\n",
       " 'become',\n",
       " 'bed',\n",
       " 'bedazzle',\n",
       " 'bedeck',\n",
       " 'bedevil',\n",
       " 'beef',\n",
       " 'beep',\n",
       " 'beetle',\n",
       " 'befall',\n",
       " 'befit',\n",
       " 'befog',\n",
       " 'befriend',\n",
       " 'beg',\n",
       " 'beget',\n",
       " 'beggar',\n",
       " 'begin',\n",
       " 'begrudge',\n",
       " 'beguile',\n",
       " 'behave',\n",
       " 'behead',\n",
       " 'behold',\n",
       " 'behoove',\n",
       " 'behove',\n",
       " 'belabor',\n",
       " 'belabour',\n",
       " 'belay',\n",
       " 'belch',\n",
       " 'belie',\n",
       " 'believe',\n",
       " 'belittle',\n",
       " 'bellow',\n",
       " 'belly',\n",
       " 'bellyache',\n",
       " 'belong',\n",
       " 'belt',\n",
       " 'bemoan',\n",
       " 'bemuse',\n",
       " 'benchmark',\n",
       " 'bend',\n",
       " 'benefit',\n",
       " 'bequeath',\n",
       " 'berate',\n",
       " 'bereave',\n",
       " 'berth',\n",
       " 'beseech',\n",
       " 'beset',\n",
       " 'besiege',\n",
       " 'besmirch',\n",
       " 'bespatter',\n",
       " 'bespeak',\n",
       " 'best',\n",
       " 'bestir',\n",
       " 'bestow',\n",
       " 'bestride',\n",
       " 'bet',\n",
       " 'betake',\n",
       " 'betide',\n",
       " 'betoken',\n",
       " 'betray',\n",
       " 'better',\n",
       " 'bewail',\n",
       " 'beware',\n",
       " 'bewilder',\n",
       " 'bewitch',\n",
       " 'bias',\n",
       " 'bicker',\n",
       " 'bicycle',\n",
       " 'bid',\n",
       " 'bide',\n",
       " 'biff',\n",
       " 'bifurcate',\n",
       " 'big',\n",
       " 'bike',\n",
       " 'bilk',\n",
       " 'bill',\n",
       " 'billet',\n",
       " 'billow',\n",
       " 'bin',\n",
       " 'bind',\n",
       " 'binge',\n",
       " 'biodegrade',\n",
       " 'bird',\n",
       " 'bisect',\n",
       " 'bitch',\n",
       " 'bite',\n",
       " 'bitmap',\n",
       " 'bivouac',\n",
       " 'bivvy',\n",
       " 'blab',\n",
       " 'blabber',\n",
       " 'black',\n",
       " 'blackball',\n",
       " 'blacken',\n",
       " 'blacklist',\n",
       " 'blackmail',\n",
       " 'blag',\n",
       " 'blame',\n",
       " 'blanch',\n",
       " 'blank',\n",
       " 'blanket',\n",
       " 'blare',\n",
       " 'blaspheme',\n",
       " 'blast',\n",
       " 'blather',\n",
       " 'blaze',\n",
       " 'blazon',\n",
       " 'bleach',\n",
       " 'bleat',\n",
       " 'bleed',\n",
       " 'bleep',\n",
       " 'blemish',\n",
       " 'blench',\n",
       " 'blend',\n",
       " 'bless',\n",
       " 'blight',\n",
       " 'blind',\n",
       " 'blindfold',\n",
       " 'blindfolded',\n",
       " 'blindside',\n",
       " 'blink',\n",
       " 'bliss',\n",
       " 'blister',\n",
       " 'blitz',\n",
       " 'bloat',\n",
       " 'block',\n",
       " 'blockade',\n",
       " 'blog',\n",
       " 'blood',\n",
       " 'bloom',\n",
       " 'bloop',\n",
       " 'blossom',\n",
       " 'blot',\n",
       " 'blow',\n",
       " 'blub',\n",
       " 'blubber',\n",
       " 'bludge',\n",
       " 'bludgeon',\n",
       " 'bluff',\n",
       " 'blunder',\n",
       " 'blunt',\n",
       " 'blur',\n",
       " 'blurt',\n",
       " 'blush',\n",
       " 'bluster',\n",
       " 'board',\n",
       " 'boast',\n",
       " 'bob',\n",
       " 'bobble',\n",
       " 'bode',\n",
       " 'bodge',\n",
       " 'bog',\n",
       " 'boggle',\n",
       " 'boil',\n",
       " 'bolster',\n",
       " 'bolt',\n",
       " 'bomb',\n",
       " 'bombard',\n",
       " 'bond',\n",
       " 'bone',\n",
       " 'bonk',\n",
       " 'boo',\n",
       " 'boob',\n",
       " 'boogie',\n",
       " 'book',\n",
       " 'bookmark',\n",
       " 'boom',\n",
       " 'boomerang',\n",
       " 'boost',\n",
       " 'boot',\n",
       " 'bootleg',\n",
       " 'booze',\n",
       " 'bop',\n",
       " 'border',\n",
       " 'bore',\n",
       " 'born',\n",
       " 'borrow',\n",
       " 'boss',\n",
       " 'botch',\n",
       " 'bother',\n",
       " 'bottle',\n",
       " 'bottleful',\n",
       " 'bottom',\n",
       " 'bounce',\n",
       " 'bound',\n",
       " 'bow',\n",
       " 'bowdlerise',\n",
       " 'bowdlerize',\n",
       " 'bowl',\n",
       " 'bowlful',\n",
       " 'box',\n",
       " 'boycott',\n",
       " 'braai',\n",
       " 'brace',\n",
       " 'braces',\n",
       " 'bracket',\n",
       " 'brag',\n",
       " 'braid',\n",
       " 'brain',\n",
       " 'brainstorm',\n",
       " 'brainwash',\n",
       " 'braise',\n",
       " 'brake',\n",
       " 'branch',\n",
       " 'brand',\n",
       " 'brandish',\n",
       " 'brave',\n",
       " 'brawl',\n",
       " 'bray',\n",
       " 'brazen',\n",
       " 'breach',\n",
       " 'break',\n",
       " 'breakfast',\n",
       " 'breast',\n",
       " 'breastfeed',\n",
       " 'breathalyse',\n",
       " 'breathalyze',\n",
       " 'breathe',\n",
       " 'breed',\n",
       " 'breeze',\n",
       " 'brew',\n",
       " 'bribe',\n",
       " 'brick',\n",
       " 'bridge',\n",
       " 'bridle',\n",
       " 'brief',\n",
       " 'brighten',\n",
       " 'brim',\n",
       " 'bring',\n",
       " 'bristle',\n",
       " 'broach',\n",
       " 'broadcast',\n",
       " 'broaden',\n",
       " 'broadside',\n",
       " 'broil',\n",
       " 'broker',\n",
       " 'brood',\n",
       " 'brook',\n",
       " 'browbeat',\n",
       " 'brown',\n",
       " 'browse',\n",
       " 'bruise',\n",
       " 'bruit',\n",
       " 'brush',\n",
       " 'brutalise',\n",
       " 'brutalize',\n",
       " 'bubble',\n",
       " 'buck',\n",
       " 'bucket',\n",
       " 'bucketful',\n",
       " 'buckle',\n",
       " 'bud',\n",
       " 'buddy',\n",
       " 'budge',\n",
       " 'budget',\n",
       " 'buff',\n",
       " 'buffer',\n",
       " 'buffet',\n",
       " 'bug',\n",
       " 'bugger',\n",
       " 'build',\n",
       " 'bulge',\n",
       " 'bulk',\n",
       " 'bulldoze',\n",
       " 'bullshit',\n",
       " 'bully',\n",
       " 'bum',\n",
       " 'bumble',\n",
       " 'bump',\n",
       " 'bunch',\n",
       " 'bundle',\n",
       " 'bung',\n",
       " 'bungle',\n",
       " 'bunk',\n",
       " 'bunker',\n",
       " 'bunt',\n",
       " 'buoy',\n",
       " 'burble',\n",
       " 'burden',\n",
       " 'burgeon',\n",
       " 'burglarize',\n",
       " 'burgle',\n",
       " 'burn',\n",
       " 'burnish',\n",
       " 'burp',\n",
       " 'burrow',\n",
       " 'burst',\n",
       " 'bury',\n",
       " 'bus',\n",
       " 'bushwhack',\n",
       " 'busk',\n",
       " 'bust',\n",
       " 'bustle',\n",
       " 'busy',\n",
       " 'butcher',\n",
       " 'butt',\n",
       " 'butter',\n",
       " 'button',\n",
       " 'buttonhole',\n",
       " 'buttress',\n",
       " 'buy',\n",
       " 'buzz',\n",
       " 'buzzing',\n",
       " 'bypass',\n",
       " 'cable',\n",
       " 'cache',\n",
       " 'cackle',\n",
       " 'caddie',\n",
       " 'cadge',\n",
       " 'cage',\n",
       " 'cajole',\n",
       " 'cake',\n",
       " 'calcify',\n",
       " 'calculate',\n",
       " 'calibrate',\n",
       " 'call',\n",
       " 'calm',\n",
       " 'calve',\n",
       " 'camouflage',\n",
       " 'camp',\n",
       " 'campaign',\n",
       " 'can',\n",
       " 'canalise',\n",
       " 'canalize',\n",
       " 'cancel',\n",
       " 'cane',\n",
       " 'cannibalise',\n",
       " 'cannibalize',\n",
       " 'cannon',\n",
       " 'cannulate',\n",
       " 'canoe',\n",
       " 'canonise',\n",
       " 'canonize',\n",
       " 'canoodle',\n",
       " 'canst',\n",
       " 'cant',\n",
       " 'canter',\n",
       " 'canvass',\n",
       " 'cap',\n",
       " 'caper',\n",
       " 'capitalise',\n",
       " 'capitalize',\n",
       " 'capitulate',\n",
       " 'capsize',\n",
       " 'captain',\n",
       " 'caption',\n",
       " 'captivate',\n",
       " 'capture',\n",
       " 'caramelise',\n",
       " 'caramelize',\n",
       " 'carbonise',\n",
       " 'carbonize',\n",
       " 'carburise',\n",
       " 'carburize',\n",
       " 'card',\n",
       " 'care',\n",
       " 'careen',\n",
       " 'career',\n",
       " 'caress',\n",
       " 'caricature',\n",
       " 'carjack',\n",
       " 'carol',\n",
       " 'carom',\n",
       " 'carouse',\n",
       " 'carp',\n",
       " 'carpet',\n",
       " 'carpool',\n",
       " 'carry',\n",
       " 'cart',\n",
       " 'cartwheel',\n",
       " 'carve',\n",
       " 'cascade',\n",
       " 'case',\n",
       " 'cash',\n",
       " 'cashier',\n",
       " 'casserole',\n",
       " 'cast',\n",
       " 'castigate',\n",
       " 'castrate',\n",
       " 'catalog',\n",
       " 'catalogue',\n",
       " 'catalyse',\n",
       " 'catalyze',\n",
       " 'catapult',\n",
       " 'catch',\n",
       " 'categorise',\n",
       " 'categorize',\n",
       " 'cater',\n",
       " 'caterwaul',\n",
       " 'catnap',\n",
       " 'caucus',\n",
       " 'caulk',\n",
       " 'cause',\n",
       " 'cauterise',\n",
       " 'cauterize',\n",
       " 'caution',\n",
       " 'cave',\n",
       " 'cavil',\n",
       " 'cavort',\n",
       " 'caw',\n",
       " 'cc',\n",
       " 'cease',\n",
       " 'cede',\n",
       " 'celebrate',\n",
       " 'cement',\n",
       " 'censor',\n",
       " 'censure',\n",
       " 'centralise',\n",
       " 'centralize',\n",
       " 'centre',\n",
       " 'certificate',\n",
       " 'certify',\n",
       " 'chafe',\n",
       " 'chaff',\n",
       " 'chain',\n",
       " 'chair',\n",
       " 'chalk',\n",
       " 'challenge',\n",
       " 'champ',\n",
       " 'champion',\n",
       " 'chance',\n",
       " 'change',\n",
       " 'channel',\n",
       " 'chant',\n",
       " 'chaperon',\n",
       " 'chaperone',\n",
       " 'char',\n",
       " 'characterise',\n",
       " 'characterize',\n",
       " 'charbroil',\n",
       " 'charge',\n",
       " 'chargesheet',\n",
       " 'chargrill',\n",
       " 'charm',\n",
       " 'chart',\n",
       " 'charter',\n",
       " 'chase',\n",
       " 'chasten',\n",
       " 'chastise',\n",
       " 'chat',\n",
       " 'chatter',\n",
       " 'chauffeur',\n",
       " 'cheapen',\n",
       " 'cheat',\n",
       " 'cheater',\n",
       " 'check',\n",
       " 'checkmate',\n",
       " 'cheek',\n",
       " 'cheep',\n",
       " 'cheer',\n",
       " 'cherish',\n",
       " 'chew',\n",
       " 'chicken',\n",
       " 'chide',\n",
       " 'chill',\n",
       " 'chillax',\n",
       " 'chime',\n",
       " 'chink',\n",
       " 'chip',\n",
       " 'chirp',\n",
       " 'chisel',\n",
       " 'chivvy',\n",
       " 'chlorinate',\n",
       " 'choke',\n",
       " 'chomp',\n",
       " 'choose',\n",
       " 'chop',\n",
       " 'choreograph',\n",
       " 'chortle',\n",
       " 'chorus',\n",
       " 'christen',\n",
       " 'chromakey',\n",
       " 'chronicle',\n",
       " 'chuck',\n",
       " 'chuckle',\n",
       " 'chug',\n",
       " 'chunder',\n",
       " 'chunter',\n",
       " 'churn',\n",
       " 'cinch',\n",
       " 'circle',\n",
       " 'circulate',\n",
       " 'circumcise',\n",
       " 'circumnavigate',\n",
       " 'circumscribe',\n",
       " 'circumvent',\n",
       " 'cite',\n",
       " 'civilise',\n",
       " 'civilize',\n",
       " 'clack',\n",
       " 'claim',\n",
       " 'clam',\n",
       " 'clamber',\n",
       " 'clamor',\n",
       " 'clamour',\n",
       " 'clamp',\n",
       " 'clang',\n",
       " 'clank',\n",
       " 'clap',\n",
       " 'clarify',\n",
       " 'clash',\n",
       " 'clasp',\n",
       " 'class',\n",
       " 'classify',\n",
       " 'clatter',\n",
       " 'claw',\n",
       " 'clean',\n",
       " 'cleanse',\n",
       " 'clear',\n",
       " 'cleave',\n",
       " 'clench',\n",
       " 'clerk',\n",
       " 'click',\n",
       " 'climax',\n",
       " 'climb',\n",
       " 'clinch',\n",
       " 'cling',\n",
       " 'clink',\n",
       " 'clinking',\n",
       " 'clip',\n",
       " 'cloak',\n",
       " 'clobber',\n",
       " 'clock',\n",
       " 'clog',\n",
       " 'clone',\n",
       " 'clonk',\n",
       " 'close',\n",
       " 'closet',\n",
       " 'clot',\n",
       " 'clothe',\n",
       " 'cloud',\n",
       " 'clout',\n",
       " 'clown',\n",
       " 'club',\n",
       " 'cluck',\n",
       " 'clue',\n",
       " 'clump',\n",
       " 'clunk',\n",
       " 'cluster',\n",
       " 'clutch',\n",
       " 'clutter',\n",
       " 'coach',\n",
       " 'coagulate',\n",
       " 'coalesce',\n",
       " 'coarsen',\n",
       " 'coast',\n",
       " 'coat',\n",
       " 'coax',\n",
       " 'cobble',\n",
       " 'cock',\n",
       " 'cocoon',\n",
       " 'coddle',\n",
       " 'code',\n",
       " 'codify',\n",
       " 'coerce',\n",
       " 'coexist',\n",
       " 'cogitate',\n",
       " 'cohabit',\n",
       " 'cohere',\n",
       " 'coil',\n",
       " 'coin',\n",
       " 'coincide',\n",
       " 'collaborate',\n",
       " 'collapse',\n",
       " 'collar',\n",
       " 'collate',\n",
       " 'collect',\n",
       " 'collectivise',\n",
       " 'collectivize',\n",
       " 'collide',\n",
       " 'colligate',\n",
       " 'collocate',\n",
       " 'collude',\n",
       " 'colonise',\n",
       " 'colonize',\n",
       " 'colorize',\n",
       " 'colour',\n",
       " 'comb',\n",
       " 'combat',\n",
       " 'combine',\n",
       " 'combust',\n",
       " 'come',\n",
       " 'comfort',\n",
       " 'command',\n",
       " 'commandeer',\n",
       " 'commemorate',\n",
       " 'commence',\n",
       " 'commend',\n",
       " 'comment',\n",
       " 'commentate',\n",
       " 'commercialise',\n",
       " 'commercialize',\n",
       " 'commingle',\n",
       " 'commiserate',\n",
       " 'commission',\n",
       " 'commit',\n",
       " 'commune',\n",
       " 'communicate',\n",
       " 'commute',\n",
       " 'compact',\n",
       " 'compare',\n",
       " 'compartmentalise',\n",
       " 'compartmentalize',\n",
       " 'compel',\n",
       " 'compensate',\n",
       " 'compete',\n",
       " 'compile',\n",
       " 'complain',\n",
       " 'complement',\n",
       " 'complete',\n",
       " 'complicate',\n",
       " 'compliment',\n",
       " 'comply',\n",
       " 'comport',\n",
       " 'compose',\n",
       " 'compost',\n",
       " 'compound',\n",
       " 'comprehend',\n",
       " 'compress',\n",
       " 'comprise',\n",
       " 'compromise',\n",
       " 'compute',\n",
       " 'computerise',\n",
       " 'computerize',\n",
       " 'con',\n",
       " 'conceal',\n",
       " 'concede',\n",
       " 'conceive',\n",
       " 'concentrate',\n",
       " 'conceptualise',\n",
       " 'conceptualize',\n",
       " 'concern',\n",
       " ...]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(set_verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'any',\n",
       " 'attempting',\n",
       " 'before',\n",
       " 'blocking',\n",
       " 'calling',\n",
       " 'calls',\n",
       " 'causing',\n",
       " 'could',\n",
       " 'declaring',\n",
       " 'did',\n",
       " 'forcing',\n",
       " 'him',\n",
       " 'instead',\n",
       " 'intended',\n",
       " 'kept',\n",
       " 'making',\n",
       " 'meant',\n",
       " 'moves',\n",
       " 'passing',\n",
       " 'prevented',\n",
       " 'preventing',\n",
       " 'rather',\n",
       " 'referring',\n",
       " 'saying',\n",
       " 'started',\n",
       " 'stoplight',\n",
       " 'stopped',\n",
       " 'stopping',\n",
       " 'stops',\n",
       " 'them',\n",
       " 'they',\n",
       " 'threatened',\n",
       " 'threatening',\n",
       " 'to',\n",
       " 'trains',\n",
       " 'tried',\n",
       " 'trying',\n",
       " 'urging',\n",
       " 'wanted',\n",
       " 'way',\n",
       " 'went',\n",
       " 'when',\n",
       " 'without',\n",
       " 'would'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depth = 3\n",
    "set(word for word in gen_thresh_domain([\"stop\",\"stoplight\"], depth) if word not in set_verbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our verbs (from the `verbsList.text` placed in the **same level** as this file), we will remove **all verbs** from the domain and the captions and see if we get better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doneA\n",
      "Dist Similar A to Domain: with Depth 4\n",
      "776.0975\n",
      "\n",
      "\n",
      "Dist Similar B to Domain: with Depth 4\n",
      "755.53284\n",
      "\n",
      "\n",
      "Dist Different A to Domain: with Depth 4\n",
      "926.8762\n",
      "_____________________________\n",
      "Sim A, Sim B  = 20.56463623046875\n",
      "Sim A, Diff A= 150.77874755859375\n",
      "Sim B, Diff A  = 171.3433837890625\n"
     ]
    }
   ],
   "source": [
    "depth = 4\n",
    "aa = convert_to_embeddings(set(word for word in gen_thresh_domain([\"stop\",\"stoplight\"], depth) if word not in set_verbs))\n",
    "print(\"doneA\")\n",
    "dd = convert_to_embeddings(set(word for word in gen_thresh_domain([\"apple\",\"banana\"], depth) if word not in set_verbs))\n",
    "# print(\"Distances between similar_image_a, domain: \")\n",
    "# print(calcuate_distances(similar_image_a,bb))\n",
    "\n",
    "# print(\"\\n\")\n",
    "# print(\"Distances between different_image_a, domain: \")\n",
    "# print(calcuate_distances(different_image_a,bb))\n",
    "\n",
    "# print(\"\\n\")\n",
    "# print(\"Distances between similar_image_b, domain: \")\n",
    "# print(calcuate_distances(similar_image_b,bb))\n",
    "\n",
    "# print(\"\\n\")\n",
    "# print(\"Printing the sums of each:\")\n",
    "# print(\"\\n\")\n",
    "print(\"Dist Similar A to Domain: with Depth {}\".format(depth))\n",
    "a = calcuate_distance(similar_image_a,aa)\n",
    "print(a)\n",
    "print(\"\\n\")\n",
    "print(\"Dist Similar B to Domain: with Depth {}\".format(depth))\n",
    "b = calcuate_distance(similar_image_b,aa)\n",
    "print(b)\n",
    "print(\"\\n\")\n",
    "print(\"Dist Different A to Domain: with Depth {}\".format(depth))\n",
    "c = calcuate_distance(different_image_a,aa)\n",
    "print(c)\n",
    "\n",
    "print(\"_____________________________\")\n",
    "print(\"Sim A, Sim B  = {}\".format(abs(b-a)))\n",
    "print(\"Sim A, Diff A= {}\".format(abs(c-a)))\n",
    "print(\"Sim B, Diff A  = {}\".format(abs(c-b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'near', 'stop', 'buildings', 'traffic', 'road', 'light', 'sign', 'sitting'}\n",
      "{'stop', 'lights', 'pole', 'street', 'sign'}\n",
      "{'bunch', 'hanging', 'bananas'}\n",
      "{'bananas', 'unripe', 'lots', 'products', 'store'}\n",
      "2264.1357\n",
      "2203.3262\n",
      "1913.2384\n",
      "1774.3337\n",
      "--------------------\n",
      "2232.8982\n",
      "2280.894\n",
      "1888.7606\n",
      "1835.6624\n"
     ]
    }
   ],
   "source": [
    "#Here we will do distances to domai instead, since this is already the baseline\n",
    "print(gen_cap_terms(test2))\n",
    "print(gen_cap_terms(test))\n",
    "print(gen_cap_terms(diff1))\n",
    "print(gen_cap_terms(diff2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cap_a = convert_to_embeddings(gen_cap_terms(test))\n",
    "cap_b = convert_to_embeddings(gen_cap_terms(test2))\n",
    "cap_d = convert_to_embeddings(gen_cap_terms(diff1))\n",
    "cap_e= convert_to_embeddings(gen_cap_terms(diff2))\n",
    "\n",
    "a = calcuate_distance(dd,cap_a)\n",
    "print(a)\n",
    "a = calcuate_distance(dd,cap_b)\n",
    "print(a)\n",
    "a = calcuate_distance(dd,cap_d)\n",
    "print(a)\n",
    "a = calcuate_distance(dd,cap_e)\n",
    "print(a)\n",
    "\n",
    "print(\"--------------------\")\n",
    "setty = convert_to_embeddings(set([\"sign\",\"light\",\"road\",\"traffic\"]))\n",
    "a = calcuate_distance(dd,setty)\n",
    "print(a)\n",
    "setty = convert_to_embeddings(set([\"lights\",\"pole\",\"sign\",\"street\"]))\n",
    "a = calcuate_distance(dd,setty)\n",
    "print(a)\n",
    "setty = convert_to_embeddings(set([\"bananas\",\"food\",\"bunch\"]))\n",
    "a = calcuate_distance(dd,setty)\n",
    "print(a)\n",
    "setty = convert_to_embeddings(set([\"bananas\",\"store\",\"unripe\"]))\n",
    "a = calcuate_distance(dd,setty)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Observations on Verbs\n",
    "\n",
    "When we don't get rid of verbs, the general **distances are larger** and the **distances between different captions of different images are larger** but sometimes those distances can be sporadically different even if they are from the *same image*.\n",
    "\n",
    "\n",
    "When we do get rid of the verbs, **the overall distances are smaller** and the **distances between different captions of different images is smaller** but these distances among captions from the same images **is a lot less**\n",
    "\n",
    "So to summarize **having verbs gives greater absolute distances** whereas **not having verbs gives tighter distances among the same image captions**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
